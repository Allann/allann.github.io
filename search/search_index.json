{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hi there, I'm Allan","text":"<p>I'd like to welcome you to my personal github page where I share thoughts and solutions to specific tasks.  Have a look at the blog before you leave there might be something interesting to read.</p>"},{"location":"about/","title":"About","text":"<p>I am an experienced senior software engineer and mentor with over two decades of professional expertise. Presently, I am employed at JJ Richards &amp; Sons, where my focus lies in the development of enterprise applications utilizing technologies such as .NET, Blazor, and Azure. I thrive on challenging projects and possess the proficiency to transform initial proposals into meticulously crafted, well-defined, coded, and thoroughly tested solutions.</p>"},{"location":"about/#github-stats","title":"GitHub Stats","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/06/20/the-subtle-cost-of-ai-dependence/","title":"The Subtle Cost of AI Dependence","text":"<p>Artificial intelligence continues to reshape our industry\u2014often for the better. It increases productivity, accelerates innovation, and reduces repetitive work. However, this transformation is not without its drawbacks. One emerging concern is the quiet erosion of human expertise, particularly when AI is used as a substitute for skill rather than a tool to enhance it.</p>","tags":["code"]},{"location":"blog/2025/06/20/the-subtle-cost-of-ai-dependence/#leveraging-ai-with-experience","title":"Leveraging AI with Experience","text":"<p>Experienced professionals understand how to use AI effectively. For them, AI acts as a force multiplier\u2014they evaluate, refine, and validate its outputs. Their depth of knowledge allows them to identify shortcomings, maintain quality, and guide both colleagues and the AI systems themselves through feedback and correction.</p>","tags":["code"]},{"location":"blog/2025/06/20/the-subtle-cost-of-ai-dependence/#the-risk-to-emerging-talent","title":"The Risk to Emerging Talent","text":"<p>Less experienced team members often use AI differently. Without the same contextual grounding or technical depth, they are more likely to accept AI-generated content at face value, especially with the positive and confident tone most LLMs use regardless of the accuracy portrayed. This is a natural part of early-career learning: we grow through practice, feedback, and mistakes.</p> <p>However, when AI becomes a crutch rather than a collaborator, that learning can stall. Junior employees may not be equipped to question, improve, or even fully understand what the AI produces. This can result in output that is technically functional but lacks nuance, whether in code, documentation, or strategic thinking.</p> <p>The consequences extend beyond individuals. Poor-quality inputs can degrade the performance of AI systems themselves. When these systems rely on feedback loops informed by mediocre human guidance, or the previous iteration deciding accuracy, the overall quality of outputs can diminish over time, creating a negative spiral.</p>","tags":["code"]},{"location":"blog/2025/06/20/the-subtle-cost-of-ai-dependence/#a-hidden-but-growing-problem","title":"A Hidden but Growing Problem","text":"<p>Today, this risk is buffered by experienced professionals still in the workforce. But as they retire or shift into non-technical roles, organizations may find themselves relying on teams that have not fully developed the foundational skills needed to lead effectively.</p> <p>This transition, if left unaddressed, could lead to an organization-wide erosion of capability. The drive for short-term efficiency, automating tasks, and accelerating delivery, can obscure the long-term cost: a workforce less able to reason critically, troubleshoot effectively, or innovate meaningfully.</p>","tags":["code"]},{"location":"blog/2025/06/20/the-subtle-cost-of-ai-dependence/#everyday-parallels","title":"Everyday Parallels","text":"<p>This trend isn\u2019t unique to software. Consider how drivers increasingly rely on GPS. While convenient, over-reliance can weaken our spatial reasoning skills. A 2020 study in Nature found that habitual GPS users not only showed reduced activity in brain areas related to navigation but also had less grey matter in those regions. Over time, critical thinking and intuitive problem-solving atrophy when they\u2019re no longer exercised.</p> <p>Similarly, in the corporate world, relying on AI to do our thinking for us, without truly understanding the logic behind the output, leaves us less prepared to adapt when tools fail or edge cases emerge.</p> <p>This is not a new phenomenon. During the Industrial Revolution, a divide emerged between machine operators and the expert craftspeople who truly understood the manufacturing process. According to research by Kelly, Mokyr, and \u00d3 Gr\u00e1da, while fewer in number, highly trained specialists emerged as essential to advancing and sustaining industrial efficiency.</p>","tags":["code"]},{"location":"blog/2025/06/20/the-subtle-cost-of-ai-dependence/#how-we-preserve-expertise-in-an-ai-age","title":"How We Preserve Expertise in an AI Age","text":"<p>Organizations must take deliberate steps to balance automation with skill development. Some practical strategies could include:</p> <ol> <li> <p>Structured Mentorship:    Facilitate formal mentoring programs where experienced professionals guide junior staff, ensuring that institutional knowledge is actively passed down.</p> </li> <li> <p>Ongoing Learning Investment:    Commit to continuous professional development. Encourage not only learning new tools but deepening understanding of foundational principles.</p> </li> <li> <p>Hire for Curiosity:    Seek out candidates who ask \u201cwhy,\u201d not just \u201chow.\u201d A genuine desire to understand systems, not just use them, is a better long-term indicator of adaptability and contribution.</p> </li> <li> <p>Adopt a Hybrid Approach:    AI should augment human judgment, not replace it. Encourage employees to critically evaluate AI output, fostering a mindset where technology is a collaborator, not an authority.</p> </li> <li> <p>Establish Quality Assurance Frameworks:    Create dedicated teams to review AI-generated work. These should include experienced staff responsible for setting and upholding quality standards, while holding all contributors accountable.</p> </li> <li> <p>Encourage Feedback Loops:    Build systems where learnings from real-world usage of AI tools are shared across teams, improving both human understanding and AI performance.</p> </li> </ol>","tags":["code"]},{"location":"blog/2025/06/20/the-subtle-cost-of-ai-dependence/#moving-forward-responsibly","title":"Moving Forward Responsibly","text":"<p>Much of today\u2019s AI discussion centers on replacement, what roles AI will eliminate, what industries it will disrupt. But we must also consider what expertise it might quietly diminish.</p> <p>To ensure long-term resilience and innovation, organizations need to foster a culture where AI is seen as a tool for amplification, not delegation. We must protect and pass on the human insight that underpins high-quality work, critical reasoning, and deep problem-solving.</p> <p>Let\u2019s stop adding disclaimers like \u201cChatGPT may make mistakes\u201d to the bottom of every tool. Instead, let\u2019s build teams that can recognize and correct those mistakes, because they understand what right looks like in the first place.</p>","tags":["code"]},{"location":"blog/2024/12/17/conventions/","title":"Conventions","text":"<p>I wanted a central place where I could look up these details as I learn them and also point others too that ask, Where I have borrowed the content for my own use, I'll include the link in the section.</p>","tags":["PR","code"]},{"location":"blog/2024/12/17/conventions/#code-review-comments","title":"Code review comments","text":"<p>See https://conventionalcomments.org/ for the original content</p>","tags":["PR","code"]},{"location":"blog/2024/12/17/conventions/#format","title":"Format","text":"<p>\\ [decorations]: \\ <p>[discussion]</p> <p>suggest using the following labels:</p> Label Description praise: Praises highlight something positive. Try to leave at least one of these comments per review. Do not leave false praise (which can actually be damaging). Do look for something to sincerely praise. nitpick: Nitpicks are trivial preference-based requests. These should be non-blocking by nature. suggestion: Suggestions propose improvements to the current subject. It's important to be explicit and clear on what is being suggested and why it is an improvement. Consider using patches and the blocking or non-blocking decorations to further communicate your intent. issue: Issues highlight specific problems with the subject under review. These problems can be user-facing or behind the scenes. It is strongly recommended to pair this comment with a suggestion. If you are not sure if a problem exists or not, consider leaving a question. todo: TODO's are small, trivial, but necessary changes. Distinguishing todo comments from issues: or suggestions: helps direct the reader's attention to comments requiring more involvement. question: Questions are appropriate if you have a potential concern but are not quite sure if it's relevant or not. Asking the author for clarification or investigation can lead to a quick resolution. thought: Thoughts represent an idea that popped up from reviewing. These comments are non-blocking by nature, but they are extremely valuable and can lead to more focused initiatives and mentoring opportunities. chore: Chores are simple tasks that must be done before the subject can be \u201cofficially\u201d accepted. Usually, these comments reference some common process. Try to leave a link to the process description so that the reader knows how to resolve the chore. note: Notes are always non-blocking and simply highlight something the reader should take note of.","tags":["PR","code"]},{"location":"blog/2025/05/29/custom-otel-metrics/","title":"Creating Meaningful Custom Metrics in .NET with OpenTelemetry and .NET Aspire Dashboard","text":"<p>Observability\u2014the ability to understand what is happening inside your software systems\u2014has become essential in today\u2019s complex applications. While logs and traces provide valuable technical information, custom metrics unlock deeper insights by focusing on what truly matters to your business.</p> <p>Throughout this article we will anchor concepts to a real\u2010world scenario taken from the waste\u2011management domain: an API that coordinates requests for video footage captured by garbage\u2011truck camera units.</p> <p>Scenario in Brief</p> <ul> <li>Requests arrive from multiple channels\u2014an internal support portal, council\u2011facing public website, and service\u2011to\u2011service calls.</li> <li>Each request identifies a truck, resolves the truck\u2019s current video\u2011provider integration, triggers the provider\u2019s API, and queues the job.</li> <li>A background worker later downloads the footage, applies watermarking, stores it on\u2011prem, and notifies the requestor by email.</li> <li>Access to the video is time\u2011boxed: downloads/views can be restricted per user type; content expires after 30 days and is deleted if never accessed within seven days to conserve storage.</li> </ul> <p>We will use this workflow to show why and how to craft business\u2011centric OpenTelemetry metrics that surface genuine operational and customer value.</p>","tags":["otel","aspire"]},{"location":"blog/2025/05/29/custom-otel-metrics/#what-you-will-learn","title":"What You Will Learn","text":"<ul> <li>Why custom metrics are vital for business\u2011focused monitoring</li> <li>How they differ from built\u2011in technical metrics</li> <li>Step\u2011by\u2011step guidance on creating custom metrics with OpenTelemetry in .NET 9+</li> <li>How to visualise those metrics in real time with the .NET Aspire Dashboard</li> <li>Proven practices to maximise the value of your metrics</li> </ul>","tags":["otel","aspire"]},{"location":"blog/2025/05/29/custom-otel-metrics/#why-custom-metrics-matter-more-than-technical-metrics-alone","title":"Why Custom Metrics Matter More Than Technical Metrics Alone","text":"<p>Application Performance Monitoring (or Management) suites refers to a commercial or open-source toolset that continuously collects and analyses telemetry (CPU, memory, request latency, error rates, traces, etc.) from your applications and infrastructure so teams can detect and diagnose performance issues in real time. These suites ship technical or infrastructure\u2011level metrics out of the box\u2014CPU usage, memory consumption, HTTP latency. These figures tell you how well the runtime is behaving, but seldom what the numbers mean for the business workflow.  Aspire uses OpenTelemetry out the box and the one we will be targeting in this article.</p>","tags":["otel","aspire"]},{"location":"blog/2025/05/29/custom-otel-metrics/#what-are-custom-metrics","title":"What Are Custom Metrics?","text":"<p>Custom metrics are values you define and emit based on domain events. In our garbage\u2011truck video service, meaningful metrics include:</p> <ul> <li><code>video_requests.registered</code>\u2013 every new footage request (tagged with <code>origin=portal|council|s2s</code>)</li> <li><code>video_jobs.queued</code> and <code>video_jobs.failed</code></li> <li><code>videos.ready</code>\u2013 footage processed, watermarked, and stored</li> <li><code>video_notifications.sent</code> (tagged with <code>userType</code> and <code>delivery=result|error</code>)</li> <li><code>videos.expired.deleted</code>\u2013 storage reclaimed after retention rules fire</li> </ul>","tags":["otel","aspire"]},{"location":"blog/2025/05/29/custom-otel-metrics/#why-choose-custom-metrics","title":"Why Choose Custom Metrics?","text":"<ul> <li>Align With Business KPIs \u2013 Instead of \u201cCPU 85 %\u201d, operations staff see \u201cbackground\u2011queue backlog = 27\u201d and can   judge SLA risk.</li> <li>Contextual Alerting \u2013 An alert on <code>videos.ready</code> dropping to zero during business hours pinpoints a real service   failure.</li> <li>Shared Language \u2013 Support, developers and council stakeholders discuss the same graphs without translating tech   jargon.</li> </ul>","tags":["otel","aspire"]},{"location":"blog/2025/05/29/custom-otel-metrics/#scenario-illustration","title":"Scenario Illustration","text":"<p>A sudden spike in CPU could be routine watermark encoding. But a rise in <code>video_jobs.failed</code> accompanied by a stall in <code>videos.ready</code> immediately flags that footage is not reaching requestors\u2014an outcome with contractual implications for councils.</p>","tags":["otel","aspire"]},{"location":"blog/2025/05/29/custom-otel-metrics/#how-to-create-custom-metrics-in-net-using-opentelemetry","title":"How to Create Custom Metrics in .NET Using OpenTelemetry","text":"<p>Aspire and OpenTelemetry provides a vendor\u2011neutral, extensible API for metrics. The high\u2011level workflow is:</p> <ol> <li>Add and configure OpenTelemetry in your .NET 9 application (worker service, API, or both).</li> <li>Define a <code>Meter</code> and instruments (counters, histograms, up\u2011down counters) that represent domain events.</li> <li>Instrument code paths \u2014 increment counters when a request is registered; observe durations while footage is being    processed.</li> <li>Export metrics (OTLP) and visualise them in the Aspire Dashboard.</li> </ol> <pre><code>// ServiceDefaults/Extensions.cs\n\nbuilder.Services.AddOpenTelemetry()\n       .WithMetrics(metrics =&gt;\n       {\n           metrics\n                  .AddAspNetCoreInstrumentation()\n                  .AddHttpClientInstrumentation();\n\n           metrics.AddMeter(VideoMetrics.Meter.Name);\n           metrics.AddOtlpExporter();\n       })       \n       // ...other trace setup\n</code></pre> <pre><code>// program.cs\n\nbuilder.Services.AddSingleton&lt;VideoMetrics&gt;();\n\n// code removed for brevity\n\napp.MapPost(\"/api/footage\", async ([FromBody] VideoRequest request, [FromServices] VideoMetrics metrics, CancellationToken ct) =&gt;\n{\n    metrics.VideoRequestsRegistered.Add(1, new(\"request.origin\", request.Source));\n\n    // \u2026business logic\u2026\n});\n</code></pre> <pre><code>// VideoMetrics.cs \u2013 strongly\u2011typed wrapper keeps metric names consistent\npublic sealed class VideoMetrics(MeterProvider meterProvider)\n{\n    public const string Name = \"Video Api\";\n\n    public static Meter Meter = new(Name, \"1.0.0\");\n\n    public Counter&lt;int&gt; VideoRequestsRegistered { get; } =\n        Meter.CreateCounter&lt;int&gt;(\"video_requests.registered\");\n\n    public Counter&lt;int&gt; VideoJobsQueued { get; } =\n        Meter.CreateCounter&lt;int&gt;(\"video_jobs.queued\");\n\n    public Counter&lt;int&gt; VideosReady { get; } =\n        Meter.CreateCounter&lt;int&gt;(\"videos.ready\");\n\n    public Histogram&lt;int&gt; VideoProcessingDurationMs { get; } =\n        Meter.CreateHistogram&lt;int&gt;(\"video_processing.duration_ms\");\n}\n</code></pre>","tags":["otel","aspire"]},{"location":"blog/2025/05/29/custom-otel-metrics/#best-practices-when-designing-custom-metrics","title":"Best Practices When Designing Custom Metrics","text":"<ul> <li>Clear Names\u2013 prefer <code>video_jobs.failed</code> over <code>counter42</code>.</li> <li>Tag Only What You Query \u2013 high\u2011cardinality tags (e.g., truck VIN) balloon storage; stick to dimensions that   matter (<code>origin</code>, <code>userType</code>).</li> <li>Emit on Meaningful Boundaries \u2013 increment <code>videos.ready</code> once per finished file, not inside a polling loop.</li> <li>Unit Consistency \u2013 store durations in milliseconds; sizes in bytes.</li> <li>Guard Against Over\u2011Instrumentation \u2013 track the video life\u2011cycle stages, not every line of code.</li> </ul>","tags":["otel","aspire"]},{"location":"blog/2025/05/29/custom-otel-metrics/#visualising-metrics-with-net-aspire-dashboard","title":"Visualising Metrics with .NET Aspire Dashboard","text":"<p>Spin up the Aspire dashboard alongside your service and you can:</p> <ul> <li>Watch live graphs of <code>video_jobs.queued</code> vs. <code>videos.ready</code> to verify the pipeline is keeping up.</li> <li>Drill into tags \u2014 filter <code>video_requests.registered</code> by <code>origin</code> to see when council portals are busiest.</li> <li>Build KPI pages \u2014 combine counters into a Video Fulfilment dashboard that shows average time\u2011to\u2011ready and   failure ratio at a glance.</li> </ul>  \ud83d\udc49\ud83c\udffb Create a bar chart comparing `videos.expired.deleted` to total storage used; sudden drops in deletions can warn that retention logic is mis\u2011configured.","tags":["otel","aspire"]},{"location":"blog/2025/05/29/custom-otel-metrics/#getting-started-checklist","title":"Getting Started \u2013 Checklist","text":"<ol> <li>List key business events (request registered, job queued, video ready, notification sent, retention processed).</li> <li>Sketch the metric instruments and tags you need.</li> <li>Add OpenTelemetry packages and register a <code>Meter</code> with those instruments.</li> <li>Increment/observe at the appropriate service boundaries.</li> <li>Export to OTLP and explore in Aspire.</li> <li>Review with support and council stakeholders, then iterate.</li> </ol>","tags":["otel","aspire"]},{"location":"blog/2025/05/29/custom-otel-metrics/#final-thoughts","title":"Final Thoughts","text":"<p>Custom metrics elevate observability beyond technical health checks by embedding business context directly into the monitoring fabric. In our garbage\u2011truck footage service that means knowing how many videos are late, which users/services generate the most requests, and how long watermarking really takes. Armed with these insights, engineering and operations teams can act proactively \u2014 long before an irate support call lands.</p> <p>OpenTelemetry plus the .NET Aspire Dashboard delivers a vendor\u2011neutral, future\u2011proof tool\u2011chain for surfacing those insights in .NET 9 and beyond. Start small, instrument the events that matter, and let the data guide your next optimisation.</p>","tags":["otel","aspire"]},{"location":"blog/2025/05/29/custom-otel-metrics/#note","title":"NOTE","text":"<p>.NET Aspire is designed primarily for local development, offering a streamlined dashboard for viewing logs, traces, and metrics with minimal configuration. However, its dashboard is generally not intended for production use due to limitations such as in-memory event storage and lack of persistent storage for telemetry data. For robust production observability, several established alternatives are commonly used.</p> <p>Leading Production-Grade Alternatives - Grafana + Prometheus + Loki + Jaeger     - Grafana provides powerful, customizable dashboards for metrics, logs, and traces.     - Prometheus is widely used for metrics collection.     - Loki handles log aggregation.     - Jaeger (or Zipkin) provides distributed tracing.     - This stack is a standard in production environments and integrates well with OpenTelemetry, which Aspire also supports. - ELK Stack (Elasticsearch, Logstash, Kibana)     - ELK is a popular choice for log aggregation, search, and visualization.     - Kibana offers advanced dashboarding capabilities for production monitoring. - Seq     - Seq is a log server with support for structured logging and, more recently, traces.     - It is used for centralized log management in .NET environments. - Cloud-hosted Services     - Azure Monitor, AWS CloudWatch, and Google Cloud Operations Suite offer managed observability solutions with dashboards, alerting, and storage.     - These services are often recommended for production deployments, especially when hosting in the cloud.</p> Tool/Stack Metrics Logs Traces Dashboard Production Ready Storage Cloud Integration Grafana + Prometheus etc. Yes Yes Yes Yes Yes Yes Yes ELK Stack No Yes No Yes Yes Yes Yes Seq No Yes Yes Yes Yes Yes Limited .NET Aspire Dashboard Yes Yes Yes Yes Limited No (in-memory) Azure (best)","tags":["otel","aspire"]},{"location":"blog/2025/05/21/domain-pipelines/","title":"Building Domain-Specific \u201cRussian-Doll\u201d Pipelines in .NET 9 \u2013 a Functional Approach with ProblemOr","text":"<p>Move the elegance of ASP.NET Core middleware into any complex back-end workflow \u2013 without <code>HttpContext</code>, without OOP builders, and without losing DI, cancellation or early-exit behaviour.</p>","tags":["dotnet","csharp","pipeline","middleware","domain"]},{"location":"blog/2025/05/21/domain-pipelines/#why-a-pipeline","title":"Why a pipeline?","text":"<p>Business workflows such as Get GPS Logs often involve many steps:</p> <ol> <li>Classify the request (Site vs Council, Commercial vs Domestic).</li> <li>Fetch way-points from a repository.</li> <li>Convert timestamps to local time.</li> <li>Constrain results to a bounding-box.</li> <li>Materialise the response.</li> </ol> <p>Each step should be independent, replaceable, testable and able to short-circuit on error or empty data \u2013 the classic \u201cRussian-doll\u201d pattern that ASP.NET Core middleware delivers for web traffic.</p>","tags":["dotnet","csharp","pipeline","middleware","domain"]},{"location":"blog/2025/05/21/domain-pipelines/#key-design-choices","title":"Key design choices","text":"Decision Rationale Immutable context record (<code>GpsLogContext</code>) No hidden state; easy to <code>with</code>-clone for functional purity. Discriminated-union result (<code>ProblemOr&lt;T&gt;</code>) One return type covers success or one-or-many errors; consumers call <code>Match</code>. Pure functions + static \u201ccompose\u201d helper No builder classes or interfaces; the pipeline itself is a first-class function. DI scope passed in the context Middleware still resolve scoped services, but the composer stays DI-agnostic.","tags":["dotnet","csharp","pipeline","middleware","domain"]},{"location":"blog/2025/05/21/domain-pipelines/#the-core-primitives","title":"The core primitives","text":"<pre><code>using ProblemOr;\n\npublic sealed record GpsLogContext(\n    RequestModel      Request,\n    ResponseModel?    Response,\n    IServiceScope     Scope,\n    CancellationToken Ct);\n\n// Pipeline delegate (mirrors aspnetcore RequestDelegate)\npublic delegate ValueTask&lt;ProblemOr&lt;GpsLogContext&gt;&gt; GpsLogDelegate(GpsLogContext ctx);\n</code></pre>","tags":["dotnet","csharp","pipeline","middleware","domain"]},{"location":"blog/2025/05/21/domain-pipelines/#a-functional-static-builder","title":"A functional static builder","text":"<pre><code>public static class DomainPipeline\n{\n    private static readonly GpsLogDelegate Terminal =\n        ctx =&gt; ValueTask.FromResult&lt;ProblemOr&lt;GpsLogContext&gt;&gt;(ctx);\n\n    public static GpsLogDelegate Compose(\n        params Func&lt;GpsLogDelegate, GpsLogDelegate&gt;[] parts)\n    {\n        var app = Terminal;\n        for (var i = parts.Length - 1; i &gt;= 0; i--)\n            app = parts[i](app);   // Russian-doll wrap\n        return app;\n    }\n}\n</code></pre> <p><code>Compose</code> returns one <code>GpsLogDelegate</code>; there is no stateful builder instance to maintain or mock.</p>","tags":["dotnet","csharp","pipeline","middleware","domain"]},{"location":"blog/2025/05/21/domain-pipelines/#middleware-components-as-pure-functions","title":"Middleware components as pure functions","text":"<pre><code>// Request classifier\nstatic Func&lt;GpsLogDelegate, GpsLogDelegate&gt; RequestClassifier =&gt;\n    next =&gt; async ctx =&gt;\n    {\n        if (!ctx.Request.TryDetermineFlags())\n            return Error.Validation(code: \"BadType\", description: \"Unknown request\");\n\n        return await next(ctx);\n    };\n\n// Way-point fetcher (shows early error exit)\nstatic Func&lt;GpsLogDelegate, GpsLogDelegate&gt; WaypointFetcher =&gt;\n    next =&gt; async ctx =&gt;\n    {\n        var repo = ctx.Scope.ServiceProvider.GetRequiredService&lt;IWaypointRepository&gt;();\n        var points = await repo.GetAsync(ctx.Request.SiteNumber, ctx.Ct);\n\n        if (points.Count == 0)\n            return Error.NotFound(description: \"No way-points\");\n\n        var updated = ctx with { Request = ctx.Request with { Waypoints = points } };\n        return await next(updated);\n    };\n\n// Time-zone resolver\nstatic Func&lt;GpsLogDelegate, GpsLogDelegate&gt; TimezoneResolver =&gt;\n    next =&gt; async ctx =&gt;\n    {\n        var tz = ctx.Scope.ServiceProvider.GetRequiredService&lt;ITimezoneService&gt;();\n        var local = await tz.ToLocalAsync(ctx.Request.UtcTime, ctx.Ct);\n\n        var updated = ctx with { Request = ctx.Request with { LocalTime = local } };\n        return await next(updated);\n    };\n\n// Bounding-box filter\nstatic Func&lt;GpsLogDelegate, GpsLogDelegate&gt; BoundingBoxFilter =&gt;\n    next =&gt; async ctx =&gt;\n    {\n        var updated = ctx with { Request = ctx.Request.FilterToBoundingBox() };\n        return await next(updated);\n    };\n\n// Response builder \u2013 terminal step, always succeeds\nstatic Func&lt;GpsLogDelegate, GpsLogDelegate&gt; ResponseBuilder =&gt;\n    _ =&gt; ctx =&gt;\n    {\n        var response = ResponseModel.Create(ctx.Request);\n        return ValueTask.FromResult&lt;ProblemOr&lt;GpsLogContext&gt;&gt;(ctx with { Response = response });\n    };\n</code></pre> <p>Add or remove steps at will; order is controlled exclusively by the <code>Compose</code> call.</p>","tags":["dotnet","csharp","pipeline","middleware","domain"]},{"location":"blog/2025/05/21/domain-pipelines/#wiring-the-pipeline-in-programcs-minimal-api-style","title":"Wiring the pipeline in Program.cs (minimal-API style)","text":"<pre><code>using Microsoft.AspNetCore.Builder;\nusing Microsoft.Extensions.DependencyInjection;\nusing ProblemOr;\n\nusing static DomainPipeline;\n\nvar builder = WebApplication.CreateBuilder(args);\n\nbuilder.Services\n       .AddScoped&lt;IWaypointRepository, WaypointRepository&gt;()\n       .AddScoped&lt;ITimezoneService, TimezoneService&gt;();\n\n// Build the single function at start-up\nGpsLogDelegate gpsPipeline = Compose(\n    RequestClassifier,\n    WaypointFetcher,\n    TimezoneResolver,\n    BoundingBoxFilter,\n    ResponseBuilder);\n\nvar app = builder.Build();\n\napp.MapPost(\"/gpslogs\", async (RequestModel req, IServiceProvider sp, CancellationToken ct) =&gt;\n{\n    await using var scope = sp.CreateAsyncScope();\n    var seed = new GpsLogContext(req, null, scope, ct);\n\n    var result = await gpsPipeline(seed);\n\n    return result.Match(\n        ok     =&gt; Results.Ok(ok.Response),\n        errors =&gt; Results.Problem(title: \"GPS log error\",\n                                  statusCode: 400,\n                                  detail: string.Join(\" | \", errors.Select(e =&gt; e.Description))));\n});\n\nawait app.RunAsync();\n</code></pre> <ul> <li>DI scope is created per request, keeping repository and service lifetimes correct.</li> <li>All middleware run in-memory; no <code>HttpContext</code>, no Kestrel overhead.</li> <li>Early failures propagate as a single 400 response with an aggregated error message.</li> </ul>","tags":["dotnet","csharp","pipeline","middleware","domain"]},{"location":"blog/2025/05/21/domain-pipelines/#unit-testing-a-component-xunit-nsubstitute","title":"Unit-testing a component (xUnit + NSubstitute)","text":"<pre><code>[Fact]\npublic async Task WaypointFetcher_returns_NotFound_when_empty()\n{\n    // Arrange\n    var repo = Substitute.For&lt;IWaypointRepository&gt;();\n    repo.GetAsync(\"05\", Arg.Any&lt;CancellationToken&gt;()).Returns([]);\n\n    var services = new ServiceCollection().AddSingleton(repo).BuildServiceProvider();\n    await using var scope = services.CreateAsyncScope();\n\n    var ctx  = new GpsLogContext(new(\"05\"), null, scope, default);\n\n    // Act\n    var result = await WaypointFetcher(_ =&gt; throw new Exception(\"Next should not run\"))(ctx);\n\n    // Assert\n    Assert.True(result.IsError);\n    Assert.Equal(\"No way-points\", result.FirstError.Description);\n}\n</code></pre> <p>Because each middleware is a pure function, testing entails no test-server fixtures.</p>","tags":["dotnet","csharp","pipeline","middleware","domain"]},{"location":"blog/2025/05/21/domain-pipelines/#performance-and-scaling-notes","title":"Performance and scaling notes","text":"<ul> <li>No reflection \u2013 the pipeline is a pre-composed delegate chain.</li> <li>Zero allocations per step \u2013 except when <code>with</code> creates a new record instance (which is unavoidable for   immutability).</li> <li>Parallel runs \u2013 the delegate is thread-safe; every invocation receives its own <code>GpsLogContext</code>.</li> </ul> <p>If you need to reuse singleton configuration inside a component, capture it when you declare the lambda:</p> <pre><code>var staticOptions = builder.Configuration.GetSection(\"Gps\").Get&lt;GpsOptions&gt;();\nFunc&lt;GpsLogDelegate, GpsLogDelegate&gt; ConfigInjector =\n    next =&gt; ctx =&gt; next(ctx with { Request = ctx.Request with { Options = staticOptions } });\n</code></pre>","tags":["dotnet","csharp","pipeline","middleware","domain"]},{"location":"blog/2025/05/21/domain-pipelines/#take-aways","title":"Take-aways","text":"<ul> <li>Functional composition is enough \u2013 no builder objects, no interfaces.</li> <li>ProblemOr gives strongly-typed early exits; no boolean flags or exceptions are required for control-flow. <li>DI remains first-class because each middleware receives the current scope from the context.</li> <li>The pattern mirrors ASP.NET Core middleware closely, so new team-members recognise the mental model instantly.</li>","tags":["dotnet","csharp","pipeline","middleware","domain"]},{"location":"blog/2025/05/21/domain-pipelines/#ready-to-paste-code","title":"Ready-to-paste code","text":"<p>All code above fits into three small files:</p> File Contents Pipeline.cs Static <code>Pipeline.Compose</code> and the <code>GpsLogDelegate</code> alias. GpsLogContext.cs Immutable record + supporting models. Middleware.cs The five lambda components shown. <p>Drop them into any .NET 9 project and enjoy web-grade pipeline composability inside your domain logic \u2014 no custom frameworks, no accidental complexity.</p>","tags":["dotnet","csharp","pipeline","middleware","domain"]},{"location":"blog/2025/09/19/using-net-aspire-with-the-docker-publisher/","title":"Using .NET Aspire With the Docker Publisher","text":"<p>.NET Aspire is one of the most exciting additions to the .NET ecosystem in years. It delivers a modern, cloud-native development experience with strong defaults, seamless integration, and a focus on developer productivity.</p> <p>A highly requested feature is the ability to publish directly to Docker Compose. With the latest preview, this feature is available, and in this guide, I\u2019ll show you how it works.</p> <p>We\u2019ll walk through using Aspire\u2019s Docker Publisher to spin up a demo app that includes:</p> <ul> <li>A SQL Server database</li> <li>A migration service</li> <li>A minimal .NET API</li> <li>A Shell UI</li> </ul> <p>Everything runs under Docker Compose, and Aspire generates the required configuration directly from C# code. We\u2019ll cover setup, what happens behind the scenes, and how to take this configuration to a VPS or cloud host.</p>","tags":["dotnet","csharp","pipeline","docker"]},{"location":"blog/2025/09/19/using-net-aspire-with-the-docker-publisher/#the-shell-app-organisation-plugin","title":"The Shell App &amp; Organisation Plugin","text":"<p>The demo app is a typical plugin for the Shell app to highlight how Aspire handles service orchestration:</p> <ul> <li>SQL Server \u2013 used for data storage  </li> <li>API project \u2013 a minimal Web API  </li> <li>Shell \u2013 runs the UI  </li> </ul> <p>Normally, you\u2019d connect these services manually, manage configuration files, define environment variables, and write a <code>docker-compose.yml</code> file from scratch.  </p> <p>With Aspire, all of this is declared in C# inside the AppHost project:</p> <pre><code>var builder = DistributedApplication.CreateBuilder(args);\n\n// Enables Docker publisher\nbuilder.AddDockerComposeEnvironment(\"branch-environment\");\n\nvar sqlServer = builder\n    .AddSqlServer(\"branch-sql-server\", builder.CreateResourceBuilder(new ParameterResource(\"password\", _ =&gt; \"&lt;My-1st-strong password&gt;\")))\n    .WithDataVolume(\"branch-sql-server-volume\")\n    .WithLifetime(ContainerLifetime.Persistent);\n\nvar organisationDb = sqlServer.AddDatabase(\"OrganisationDB\", \"Organisations\");\nvar organisationMigrations = builder.AddProject&lt;Projects.JJs_OrganisationManagement_MigrationService&gt;(\"organisationMigrations\")\n    .WithParentRelationship(organisationDb).WithReference(organisationDb).WaitFor(organisationDb);\n\nvar organisationApi = builder.AddProject&lt;Projects.JJs_OrganisationManagement_Api&gt;(\"organisation-api\")\n    .WithReference(organisationDb).WaitForCompletion(organisationMigrations)\n    .WithSwagger().WithReDoc().WithScalar();\n\nbuilder.AddProject&lt;Projects.Shell&gt;(\"shell-ui\")\n    WithReference(organisationApi).WaitFor(organisationApi);\n\nbuilder.Build().Run();\n</code></pre> <p>This configuration provides a development environment where Aspire runs SQL Server in a container and wires up your API automatically.</p> <p>The key call here is <code>AddDockerComposeEnvironment</code>, which enables the Docker publisher. This feature is available in the <code>Aspire.Hosting.Docker</code> NuGet package (currently in preview):</p> <pre><code>Install-Package Aspire.Hosting.Docker -Version 9.4.2-preview.1.25428.12\n</code></pre>","tags":["dotnet","csharp","pipeline","docker"]},{"location":"blog/2025/09/19/using-net-aspire-with-the-docker-publisher/#installing-the-aspire-cli","title":"Installing the Aspire CLI","text":"<p>To publish your app to Docker Compose, install the Aspire CLI:</p> <pre><code>dotnet tool install --global aspire.cli --prerelease\n</code></pre> <p>Then <code>publish</code> your app:</p> <pre><code>aspire publish -o docker-compose-artifacts\n</code></pre> <p>This command scans your solution for Aspire projects and generates both a <code>docker-compose.yml</code> and a <code>.env</code> file in the specified output directory <code>docker-compose-artifacts</code>.</p> <p></p>","tags":["dotnet","csharp","pipeline","docker"]},{"location":"blog/2025/09/19/using-net-aspire-with-the-docker-publisher/#the-docker-compose-file","title":"The Docker Compose File","text":"<p>Let's examine the result:</p> <pre><code>services:\n  branch-environment-dashboard:\n    image: \"mcr.microsoft.com/dotnet/nightly/aspire-dashboard:latest\"\n    expose:\n      - \"18888\"\n      - \"18889\"\n    networks:\n      - \"aspire\"\n    restart: \"always\"\n  branch-sql-server:\n    image: \"mcr.microsoft.com/mssql/server:2022-latest\"\n    environment:\n      ACCEPT_EULA: \"Y\"\n      MSSQL_SA_PASSWORD: \"${PASSWORD}\"\n    expose:\n      - \"1433\"\n    volumes:\n      - type: \"volume\"\n        target: \"/var/opt/mssql\"\n        source: \"branch-sql-server-volume\"\n        read_only: false\n    networks:\n      - \"aspire\"\n  organisationmigrations:\n    image: \"${ORGANISATIONMIGRATIONS_IMAGE}\"\n    environment:\n      OTEL_DOTNET_EXPERIMENTAL_OTLP_EMIT_EXCEPTION_LOG_ATTRIBUTES: \"true\"\n      OTEL_DOTNET_EXPERIMENTAL_OTLP_EMIT_EVENT_LOG_ATTRIBUTES: \"true\"\n      OTEL_DOTNET_EXPERIMENTAL_OTLP_RETRY: \"in_memory\"\n      ConnectionStrings__OrganisationDB: \"Server=branch-sql-server,1433;User ID=sa;Password=${PASSWORD};TrustServerCertificate=true;Initial Catalog=Organisations\"\n      OTEL_EXPORTER_OTLP_ENDPOINT: \"http://branch-environment-dashboard:18889\"\n      OTEL_EXPORTER_OTLP_PROTOCOL: \"grpc\"\n      OTEL_SERVICE_NAME: \"organisationMigrations\"\n    depends_on:\n      branch-sql-server:\n        condition: \"service_started\"\n    networks:\n      - \"aspire\"\n  organisation-api:\n    image: \"${ORGANISATION_API_IMAGE}\"\n    environment:\n      OTEL_DOTNET_EXPERIMENTAL_OTLP_EMIT_EXCEPTION_LOG_ATTRIBUTES: \"true\"\n      OTEL_DOTNET_EXPERIMENTAL_OTLP_EMIT_EVENT_LOG_ATTRIBUTES: \"true\"\n      OTEL_DOTNET_EXPERIMENTAL_OTLP_RETRY: \"in_memory\"\n      ASPNETCORE_FORWARDEDHEADERS_ENABLED: \"true\"\n      HTTP_PORTS: \"${ORGANISATION_API_PORT}\"\n      ConnectionStrings__OrganisationDB: \"Server=branch-sql-server,1433;User ID=sa;Password=${PASSWORD};TrustServerCertificate=true;Initial Catalog=Organisations\"\n      OTEL_EXPORTER_OTLP_ENDPOINT: \"http://branch-environment-dashboard:18889\"\n      OTEL_EXPORTER_OTLP_PROTOCOL: \"grpc\"\n      OTEL_SERVICE_NAME: \"organisation-api\"\n    expose:\n      - \"${ORGANISATION_API_PORT}\"\n    depends_on:\n      organisationmigrations:\n        condition: \"service_completed_successfully\"\n    networks:\n      - \"aspire\"\n  shell-ui:\n    image: \"${SHELL_UI_IMAGE}\"\n    environment:\n      OTEL_DOTNET_EXPERIMENTAL_OTLP_EMIT_EXCEPTION_LOG_ATTRIBUTES: \"true\"\n      OTEL_DOTNET_EXPERIMENTAL_OTLP_EMIT_EVENT_LOG_ATTRIBUTES: \"true\"\n      OTEL_DOTNET_EXPERIMENTAL_OTLP_RETRY: \"in_memory\"\n      ASPNETCORE_FORWARDEDHEADERS_ENABLED: \"true\"\n      HTTP_PORTS: \"${SHELL_UI_PORT}\"\n      OTEL_EXPORTER_OTLP_ENDPOINT: \"http://branch-environment-dashboard:18889\"\n      OTEL_EXPORTER_OTLP_PROTOCOL: \"grpc\"\n      OTEL_SERVICE_NAME: \"shell-ui\"\n    expose:\n      - \"${SHELL_UI_PORT}\"\n    depends_on:\n      organisation-api:\n        condition: \"service_started\"\n    networks:\n      - \"aspire\"\nnetworks:\n  aspire:\n    driver: \"bridge\"\nvolumes:\n  branch-sql-server-volume:\n    driver: \"local\"\n</code></pre> <p>This docker-compose file is generated from the C# code we added to the <code>AppHost/Program.cs</code> file. It defines all services, images, environment variables, ports, and dependencies..</p>","tags":["dotnet","csharp","pipeline","docker"]},{"location":"blog/2025/09/19/using-net-aspire-with-the-docker-publisher/#the-env-file","title":"The <code>.env</code> file","text":"<p>Aspire also generates an <code>.env</code> file with configuration values:</p> <pre><code># Default container port for organisation-api\nORGANISATION_API_PORT=8080\n\n# Default container port for shell-ui\nSHELL_UI_PORT=8080\n\n# Parameter password\nPASSWORD=&lt;My-1st-strong password&gt;\n\n# Container image name for organisationMigrations\nORGANISATIONMIGRATIONS_IMAGE=organisationMigrations:latest\n\n# Container image name for organisation-api\nORGANISATION_API_IMAGE=organisation-api:latest\n\n# Container image name for shell-ui\nSHELL_UI_IMAGE=shell-ui:latest\n</code></pre> <p>This file contains:</p> <ul> <li>Default container ports</li> <li>Database credentials</li> <li>Container image names</li> </ul> <p>Update placeholders (such as passwords) with real values. For the images, you can build and tag them yourself, or pull from a registry.</p>","tags":["dotnet","csharp","pipeline","docker"]},{"location":"blog/2025/09/19/using-net-aspire-with-the-docker-publisher/#publishing-and-running-on-a-virtual-private-server-vps","title":"Publishing and Running on a Virtual Private Server (VPS)","text":"<p>Aspire doesn't deploy the app for you, but it gives you everything you need.</p> <p>Once the Compose file is ready, deployment to a VPS is straightforward:</p> <ol> <li>Copy the artifacts to your server using (<code>scp</code> or <code>git</code>).</li> <li>Remote into the VPS.</li> <li>Run <code>docker compose up -d</code> inside the artifact directory.</li> </ol> <p>Make sure Docker and Docker Compose are installed on the server.</p> <p>For external access, configure a reverse proxy (e.g., YARP, NGINX, or Caddy) and secure endpoints with HTTPS.</p>","tags":["dotnet","csharp","pipeline","docker"]},{"location":"blog/2025/09/19/using-net-aspire-with-the-docker-publisher/#wrapping-up","title":"Wrapping Up","text":"<p>Using .NET Aspire with Docker Compose creates a smooth developer experience and simplifies full-stack app deployment.</p> <ul> <li>Define everything in C#</li> <li>Test locally</li> <li>Publish with a single command</li> </ul> <p>No need to hand-craft Compose files or juggle infrastructure manually. Aspire keeps you cloud-agnostic, so you can move between environments with ease.</p> <p>I\u2019m genuinely excited about where this is heading \u2014 the future of cloud-native development in .NET looks bright with Aspire.</p>","tags":["dotnet","csharp","pipeline","docker"]},{"location":"blog/2025/09/19/using-net-aspire-with-the-docker-publisher/#next-steps","title":"Next Steps","text":"<p>If you\u2019d like to take this further, here are some ideas:</p> <ul> <li>\ud83d\udd17 Explore the official .NET Aspire documentation for the latest updates.</li> <li>\u2699\ufe0f Integrate Aspire publishing into a CI/CD pipeline (e.g., GitHub Actions, Azure DevOps, or GitLab).</li> <li>\u2601\ufe0f Try deploying your Compose setup to a managed environment like Azure Container Apps, AWS ECS, or Fly.io.</li> <li>\ud83d\udd12 Add a reverse proxy and HTTPS support with YARP, NGINX, or Traefik.</li> <li>\ud83d\udce6 Experiment with other services (Redis, RabbitMQ, PostgreSQL) by declaring them in C#.</li> </ul>","tags":["dotnet","csharp","pipeline","docker"]},{"location":"blog/2025/09/19/using-net-aspire-with-the-docker-publisher/#visual-overview","title":"Visual Overview","text":"<p>Here\u2019s a simple diagram of the workflow from Aspire to deployment: <pre><code>flowchart TD\n    A[AppHost in C#] --&gt; B[Aspire CLI Publish]\n    B --&gt; C[Docker Compose Artifacts]\n    C --&gt; D[Local Development]\n    C --&gt; E[VPS / Cloud Server]\n    E --&gt; F[Reverse Proxy + HTTPS]</code></pre></p> <p>This illustrates how Aspire turns your C# configuration into Docker artifacts that can be run locally or deployed remotely.</p>","tags":["dotnet","csharp","pipeline","docker"]},{"location":"blog/2025/05/19/dynamic-connection-strings/","title":"Dynamic Connection Strings in EF Core 9 Minimal APIs","text":"","tags":["dotnet","csharp","dbcontext"]},{"location":"blog/2025/05/19/dynamic-connection-strings/#altering-a-dbcontext-connection-at-runtime","title":"Altering a <code>DbContext</code> Connection at Runtime","text":"<p>In Entity Framework Core (including EF Core 9.x), the database connection is normally configured when the <code>DbContext</code> is constructed, and it isn\u2019t intended to be changed afterward. Once a <code>DbContext</code> is injected (e.g. via DI in a minimal API), its <code>DbContextOptions</code> (including the connection string) are essentially fixed. There is no built-in method to reconfigure the context\u2019s connection string after it\u2019s been created in the DI container. In other words, you cannot directly \u201cswap out\u201d the connection string of an existing context instance once it\u2019s been configured.</p> <p>That said, EF Core does provide an advanced feature to support dynamic connections if done before first use. EF Core\u2019s relational providers (SQL Server, PostgreSQL, etc.) allow you to register a context without initially specifying a connection string, then supply it at runtime. For example, <code>UseSqlServer</code> (and other <code>Use*</code> calls) have an overload that omits the connection string, in which case you must set it later before using the context. EF Core exposes the <code>DatabaseFacade.SetConnectionString()</code> extension method for this purpose. In practice, this means:</p> <ul> <li>You would configure the <code>DbContext</code> with the provider but no connection string at startup. For instance:   <code>builder.Services.AddDbContext&lt;MyContext&gt;(opt =&gt; opt.UseSqlServer());</code> (calling the parameterless <code>UseSqlServer</code>   overload). This registers the context without a concrete connection string.</li> <li>Then, at runtime (before any database operations), you can set the actual connection string on the context   instance. For example:</li> </ul> <pre><code>// Inside your endpoint or service, before using the context:\ncontext.Database.SetConnectionString(actualConnectionString);\n// Now you can use context (queries, SaveChanges, etc.)\n</code></pre> <p>This approach will dynamically point that context instance to the given connection string. However, caution is required: you must call <code>SetConnectionString</code> before the context is used to connect to the database (i.e., before any query or <code>SaveChanges</code> call). If the context has already been used (or was configured with a specific connection string initially), changing it at runtime is unsupported. In summary, EF Core technically allows dynamic assignment of the connection on a fresh context, but you cannot retroactively change an already-initialized connection string after the context has been used.</p>","tags":["dotnet","csharp","dbcontext"]},{"location":"blog/2025/05/19/dynamic-connection-strings/#supported-patterns-for-dynamic-connection-strings","title":"Supported Patterns for Dynamic Connection Strings","text":"<p>Given the constraints above, the recommended solution is to provide the correct connection string when the <code>DbContext</code> is created, rather than trying to alter it afterward. There are several patterns to achieve this in a .NET 9 Minimal API:</p>","tags":["dotnet","csharp","dbcontext"]},{"location":"blog/2025/05/19/dynamic-connection-strings/#1-use-a-dbcontext-factory-or-manual-context-creation","title":"1. Use a DbContext Factory or Manual Context Creation","text":"<p>One option is to avoid injecting the <code>DbContext</code> directly, and instead inject a factory that can create <code>DbContext</code> instances on the fly with the desired connection string. EF Core provides <code>IDbContextFactory&lt;T&gt;</code> via <code>AddDbContextFactory</code>, or you can implement your own factory. For example, a custom factory interface and implementation might look like:</p> <pre><code>public interface ITenantDbContextFactory&lt;TContext&gt; where TContext : DbContext\n{\n    TContext Create(string databaseName);\n}\n\npublic class Dcms3DbContextFactory : ITenantDbContextFactory&lt;Dcms3DbContext&gt;\n{\n    public Dcms3DbContext Create(string databaseName)\n    {\n        // Build new options with the given database name in the connection string\n        var optionsBuilder = new DbContextOptionsBuilder&lt;Dcms3DbContext&gt;();\n        optionsBuilder.UseSqlServer($\"Server=...;Database={databaseName};TrustServerCertificate=True;\");\n        return new Dcms3DbContext(optionsBuilder.Options);\n    }\n}\n</code></pre> <p>This factory constructs a new <code>Dcms3DbContext</code> with a connection string targeting the specified database (the rest of the connection details can be fixed). The calling code (e.g. an endpoint handler) would request <code>Dcms3DbContextFactory</code> from DI and use it to create a context for the current request. This pattern is essentially what one Stack Overflow answer suggested for multi-database scenarios. For example, in a minimal API endpoint:</p> <pre><code>app.MapGet(\"/data/{dbName}\", async (string dbName, Dcms3DbContextFactory factory) =&gt;\n{\n    await using var db = factory.Create(dbName);\n    var results = await db.MyEntities.ToListAsync();\n    return Results.Ok(results);\n});\n</code></pre> <p>Here, the context is created at request time with the appropriate connection string. Note: If you use <code>IDbContextFactory&lt;T&gt;</code> via <code>AddDbContextFactory</code>, it\u2019s typically registered as a singleton by default. In a dynamic scenario, you may still need to call <code>SetConnectionString</code> on the created context (since <code>AddDbContextFactory</code> usually uses a fixed configuration). Alternatively, you can register the factory as scoped and supply the dynamic connection inside the factory as shown above. In either case, you are responsible for disposing of the context instance (as shown with <code>await using var db = ...</code>).</p>","tags":["dotnet","csharp","dbcontext"]},{"location":"blog/2025/05/19/dynamic-connection-strings/#2-configure-dbcontext-per-request-via-di-scoped-configuration","title":"2. Configure DbContext per request via DI (Scoped Configuration)","text":"<p>Another approach is to leverage the dependency injection configuration to supply the connection string based on some scoped context (such as the current HTTP request or tenant). You can use the overload of <code>AddDbContext</code> that provides the <code>IServiceProvider</code> to build options. This allows you to retrieve information from other services (like configuration or HTTP context) each time a <code>DbContext</code> is created. For example, in Program.cs:</p> <pre><code>builder.Services.AddHttpContextAccessor(); // enable accessing HttpContext in DI\n\nbuilder.Services.AddDbContext&lt;Dcms3DbContext&gt;((serviceProvider, options) =&gt;\n{\n    // Get current HTTP context, route values, etc.\n    var httpContext = serviceProvider.GetRequiredService&lt;IHttpContextAccessor&gt;().HttpContext;\n    var dbName = httpContext?.Request.RouteValues[\"dbName\"] as string;\n    // Build the connection string dynamically (example assumes a base template)\n    string baseConn = configuration.GetConnectionString(\"BaseTemplate\"); // e.g. \"Server=...;Database={0};...;\"\n    var connectionString = string.Format(baseConn, dbName);\n    options.UseSqlServer(connectionString);\n});\n</code></pre> <p>In this example, whenever <code>Dcms3DbContext</code> is requested, the DI container will execute our factory lambda: it grabs the current request\u2019s <code>dbName</code> (perhaps from the URL or headers) and configures the <code>DbContextOptions</code> with the correct connection string. This effectively gives each HTTP request a context tied to its specific database. Felipe Gavil\u00e1n\u2019s blog illustrates this pattern using an HTTP header to convey a tenant ID, and the Code Maze series provides a similar example using a custom <code>IDataSourceProvider</code> service. In either case, the key is that the connection string comes from a scoped service or request data rather than being hard-coded at startup.</p> <p>This pattern requires that you have access to the necessary context (like route values, a JWT claim, or a header) by the time the <code>DbContext</code> is being constructed. In minimal APIs, route parameters are available in <code>HttpContext.Request.RouteValues</code>. Using an <code>IHttpContextAccessor</code> (registered as singleton) is a straightforward way to get this information inside the <code>AddDbContext</code> lambda. Alternatively, you could set up a dedicated scoped service (e.g. <code>ITenantService</code>) earlier in the pipeline (via middleware or an endpoint filter) that stores the chosen database name for the request, and then have the <code>DbContext</code> configuration read from that service. This approach keeps your <code>DbContext</code> registration clean, e.g.:</p> <pre><code>builder.Services.AddScoped&lt;ITenantService, TenantService&gt;();\nbuilder.Services.AddDbContext&lt;Dcms3DbContext&gt;((sp, options) =&gt;\n{\n    var tenantService = sp.GetRequiredService&lt;ITenantService&gt;();\n    string connStr = tenantService.GetCurrentTenantConnectionString();\n    options.UseSqlServer(connStr);\n});\n</code></pre> <p>In this case, <code>TenantService</code> would determine the current database (perhaps using the current user info or route data) and provide the appropriate connection string. The official EF Core documentation for multi-tenancy demonstrates this pattern: the context can accept an <code>ITenantService</code> (and perhaps <code>IConfiguration</code>) via its constructor, and use that in <code>OnConfiguring</code> to choose the connection string. The context is then added via <code>AddDbContextFactory</code> or <code>AddDbContext</code> as a scoped or transient service so that each request/tenant evaluation is fresh.</p>","tags":["dotnet","csharp","dbcontext"]},{"location":"blog/2025/05/19/dynamic-connection-strings/#3-use-onconfiguring-with-injected-configuration-alternative","title":"3. Use OnConfiguring with Injected Configuration (Alternative)","text":"<p>As mentioned above, you can also implement dynamic connection logic inside your <code>DbContext</code> class itself by overriding <code>OnConfiguring</code>. If your context\u2019s constructor has access to something like <code>IConfiguration</code> and a tenant identifier, you can call the appropriate <code>UseSqlServer</code> (or other provider) within <code>OnConfiguring</code>. For example:</p> <pre><code>public class Dcms3DbContext : DbContext\n{\n    private readonly string _connection;\n    public Dcms3DbContext(DbContextOptions&lt;Dcms3DbContext&gt; opts, IConfiguration config, ITenantService tenantSvc)\n        : base(opts)\n    {\n        // Build the connection string using the current tenant info\n        var tenantId = tenantSvc.TenantId;\n        _connection = config.GetConnectionString(tenantId); \n    }\n    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)\n    {\n        if (!string.IsNullOrEmpty(_connection))\n            optionsBuilder.UseSqlServer(_connection);\n    }\n}\n</code></pre> <p>In this setup, the context is still created per request (e.g. via a context factory or regular DI), and whenever it\u2019s constructed, it grabs the current tenant\u2019s identifier and finds the matching connection string from configuration. The <code>OnConfiguring</code> ensures the context uses that connection. This pattern achieves the same result \u2013 each <code>DbContext</code> instance is configured for the appropriate database \u2013 but keeps the logic inside the context class. The downside is that you must ensure the extra services (<code>IConfiguration</code>, <code>ITenantService</code>) are available to the context (which often means using <code>AddDbContextFactory</code> with a scoped lifetime or using constructor injection with <code>AddDbContext</code> and explicitly passing those services in). The EF Core team\u2019s guidance notes that if a user can switch tenants within the same session or scope, you might need to register the context as transient to avoid caching the old connection string. In typical request-based scoping, this isn\u2019t an issue (each request gets a new context anyway).</p>","tags":["dotnet","csharp","dbcontext"]},{"location":"blog/2025/05/19/dynamic-connection-strings/#4-multiple-dbcontext-registrations-not-typical-for-this-scenario","title":"4. Multiple <code>DbContext</code> registrations (not typical for this scenario)","text":"<p>For completeness, if the set of possible databases is known and small, some applications simply register multiple contexts (one per connection string) and choose between them. However, in your scenario (a single <code>DbContext</code> type, database name only known at request time, no multi-schema), that\u2019s not an ideal solution. It\u2019s better to use one of the dynamic approaches above rather than duplicating context types or manually choosing between many DI registrations.</p>","tags":["dotnet","csharp","dbcontext"]},{"location":"blog/2025/05/19/dynamic-connection-strings/#best-practices-for-dynamic-connection-scenarios-in-ef-core-9","title":"Best Practices for Dynamic Connection Scenarios in EF Core 9","text":"<p>When implementing dynamic connection strings, keep these best practices in mind:</p> <ul> <li> <p>Provide the connection string as early as possible: Ideally, configure the <code>DbContext</code> with the correct connection   string at creation time (per request). This avoids any need to change it afterward. Use factories or DI patterns to   supply the string based on the current context (tenant, user, etc.) before the context is used.</p> </li> <li> <p>Use scoped or transient lifetimes appropriately: For web apps, a scoped lifetime (per HTTP request) is usually   appropriate for <code>DbContext</code>. If there\u2019s a possibility a user will change the target database mid-session and you   want the same user session to fetch a new database, consider using a transient context or creating new context   instances as needed. Do not reuse the same context instance for different databases.</p> </li> <li> <p>Avoid global mutable state: Don\u2019t store the \u201ccurrent connection string\u201d in a static or singleton that is modified   per request without proper scoping. For example, the Code Maze example uses a singleton <code>DataSourceProvider</code> with a   <code>CurrentDataSource</code> property, but notes this must be carefully managed (and per-user in multi-user scenarios). A safer   approach is to keep tenant-specific info in scoped services or the <code>HttpContext</code>. This ensures threads or concurrent   requests don\u2019t interfere with each other\u2019s settings.</p> </li> <li> <p>Dispose of contexts properly: When you manually create contexts (via a factory or <code>new DbContext(options)</code>), be   sure to dispose of them after use (e.g. use <code>using</code>/<code>await using</code> blocks or let the DI scope handle disposal if the   context is resolved from the container). Each context/connection should be cleaned up after the request/unit-of-work   ends.</p> </li> <li> <p>Be mindful of connection pooling and performance: If you use DbContext pooling (<code>AddDbContextPool</code> or   <code>AddPooledDbContextFactory</code>), be aware that pooled contexts might retain state (including an open connection or a set   connection string). Pooling is generally not recommended for dynamic connection scenarios because a context from the   pool might still be tied to a previous connection. Stick to regular scoped contexts or your own factories so that each   context starts fresh with the correct connection. If performance becomes a concern, measure it \u2013 EF Core is designed   to create contexts quickly, and per-request creation is usually fine.</p> </li> <li> <p>Secure the tenant selection: Since the database is chosen at runtime, ensure that the mechanism for selecting the   database is secure and validated. For example, if you pass a database name via an API route or header, validate that   the caller is authorized for that database and that the name is valid. Avoid directly concatenating untrusted input   into connection strings without checks (to prevent connection string injection or accidental exposure of other   databases).</p> </li> <li> <p>Follow EF Core updates: EF Core (including v9) continues to improve support for multi-tenant scenarios. Keep an   eye on official docs and release notes. The official multi-tenancy guidance (for EF Core \u215e+) provides patterns that   are still applicable in EF Core 9. While EF Core doesn\u2019t have a built-in multi-tenant manager, the combination of the   techniques above (scoped config, context factories, <code>SetConnectionString</code>, etc.) is the supported way to go.</p> </li> </ul> <p>By using these patterns, you can handle a scenario where the database name is determined at request processing time. The preferred approach is to create a new <code>DbContext</code> (or configure one via DI) with the proper connection string per request, rather than trying to mutate an existing injected context. This aligns with EF Core\u2019s unit-of-work pattern and ensures each context instance talks to the correct database.</p> <p>Sources:</p> <ul> <li>Microsoft Docs \u2013 EF Core Multi-Tenancy (Multiple Databases)</li> <li>Microsoft Docs \u2013 DbContext Configuration &amp; Lifetime (for using <code>AddDbContextFactory</code> and DI)</li> <li>Code Maze \u2013 Dynamically Switching DbContext at Runtime (dynamic connection via DI)</li> <li>Stack Overflow \u2013 EF Core: dynamic connection string per tenant (custom factory example)</li> </ul>","tags":["dotnet","csharp","dbcontext"]},{"location":"blog/2023/11/02/masstransit-pt1/","title":"Utilizing MassTransit in .NET Core with RabbitMQ","text":"<p>Are you in search of an open-source distributed application framework designed for .NET, one that streamlines the development of message-based applications? MassTransit is the solution you've been seeking. It offers a straightforward yet robust means of constructing distributed systems using message-based architectural paradigms, including event-driven, publish/subscribe, and request/response patterns.</p> <p>In the next few posts, I will explain my thoughts and guide you through the process of harnessing MassTransit within a .NET-based ASP.NET Core Web API. </p> <p>By the conclusion of this series, you will be well-equipped to establish and configure a producer and consumer message service for RabbitMQ. We will also delve into some advanced RabbitMQ use cases and explore key concepts essential to effectively work with distributed solutions.</p>","tags":["design"]},{"location":"blog/2023/11/02/masstransit-pt1/#prerequisites","title":"Prerequisites","text":"<p>Before we embark on this engaging endeavor, please take a moment to review the following prerequisites to ensure a smooth and productive learning experience:</p> <ul> <li>A development environment such as Rider, Visual Studio Code, Visual Studio IDE, or any other C# text editor.</li> <li>Proficiency in C# and a solid understanding of the .NET Framework.</li> <li>Familiarity with RabbitMQ.</li> <li>A project where you can implement both the producer and consumer components.</li> <li>Docker to host and execute RabbitMQ efficiently.</li> </ul>","tags":["design"]},{"location":"blog/2023/11/02/masstransit-pt1/#an-introduction-to-masstransit","title":"An introduction to MassTransit","text":"<p>MassTransit stands as an open-source distributed application framework tailored for .NET, simplifying the development of message-based applications. It offers a straightforward yet potent avenue to construct distributed systems using message-based architectures like event-driven, publish/subscribe, and request/response patterns.</p> <p>With MassTransit, developers gain the capability to build robust, scalable, and fault-tolerant distributed applications that can seamlessly communicate across diverse platforms and languages. It accommodates various messaging transports and protocols, including RabbitMQ, Azure Service Bus, Amazon Simple Queue Service (SQS), and Apache Kafka, allowing for smooth transitions between platforms.</p> <p>MassTransit extends its utility with features like message serialization, message routing, message retry policies, and message monitoring. These attributes enable developers to create highly resilient and dependable distributed systems. Its appeal is evident.</p> <p>In summary, MassTransit stands as a potent and adaptable messaging framework, empowering .NET developers to construct scalable and resilient distributed systems via a message-based architectural paradigm\ufffdjust what we need.</p>","tags":["design"]},{"location":"blog/2023/11/02/masstransit-pt1/#why-opt-for-masstransit","title":"Why Opt for MassTransit?","text":"<p>Upon perusing the official documentation for MassTransit and gaining insight into how RabbitMQ and Azure Service Bus operate, several compelling reasons for embracing MassTransit come to light. Here are the top five motivations for choosing MassTransit:</p> <ol> <li> <p>Streamlined Development: MassTransit streamlines the programming model for crafting message-based applications, handling intricate details such as message serialization, routing, and transport. This liberates developers to concentrate on crafting business logic.</p> </li> <li> <p>Language and Platform Interoperability: MassTransit accommodates a wide array of messaging transports and protocols, simplifying the task of building distributed systems that seamlessly communicate across diverse platforms and programming languages. This opens the door to utilizing the preferred languages and platforms of individual teams while ensuring seamless intercommunication.</p> </li> <li> <p>Resilience and Fault Tolerance: MassTransit incorporates features like message retries and circuit breakers, facilitating the creation of highly resilient and fault-tolerant distributed systems. These features guarantee reliable message delivery even in the face of network disruptions and other challenges.</p> </li> <li> <p>Scalability: MassTransit is engineered to support the scaling of distributed systems with ease. It offers features like load balancing and partitioning, enabling the handling of substantial message volumes and their distribution across multiple nodes.</p> </li> <li> <p>Open-Source and Community-Driven: As an open-source project with an active community of contributors, MassTransit offers access to a wide range of features, bug fixes, and enhancements contributed by community members. This collaborative spirit enhances its utility.</p> </li> </ol>","tags":["design"]},{"location":"blog/2023/11/02/masstransit-pt1/#a-quick-introduction-to-rabbitmq","title":"A quick introduction to RabbitMQ","text":"<p>Before we dive into the practical implementation, it's crucial to establish a foundational understanding of RabbitMQ, including the intricacies of queues and exchanges.</p> <p>RabbitMQ serves as open-source message broker software, providing a messaging system for the exchange of messages between distinct applications and services. It stands as a robust and scalable messaging platform widely embraced in distributed systems.</p> <p>RabbitMQ operates by implementing the Advanced Message Queuing Protocol (AMQP), a standardized messaging protocol for message-oriented middleware. This software supports various messaging patterns, including publish/subscribe, request/response, and work queues. It provides message queues, exchanges, and bindings, enabling different applications and services to communicate in a distributed environment.</p> <p>RabbitMQ enjoys compatibility with a wide spectrum of programming languages and platforms, encompassing Java, .NET, Python, Ruby, and more. Furthermore, it supports various messaging protocols, including AMQP, STOMP, MQTT, and HTTP.</p>","tags":["design"]},{"location":"blog/2023/11/02/masstransit-pt1/#reasons-to-leverage-rabbitmq","title":"Reasons to Leverage RabbitMQ","text":"<p>RabbitMQ emerges as a dependable and scalable messaging platform designed for distributed systems. Here are key features that make RabbitMQ a compelling choice:</p> <ol> <li> <p>High Availability: RabbitMQ supports clustering, fostering high availability and fault tolerance by connecting multiple nodes.</p> </li> <li> <p>Routing: RabbitMQ facilitates diverse routing algorithms and message exchange types, ensuring messages reach their intended destinations accurately.</p> </li> <li> <p>Message Durability: By persisting messages to disk, RabbitMQ ensures message durability, preventing message loss even in the event of system failures.</p> </li> <li> <p>Security: RabbitMQ offers robust security measures, including SSL/TLS encryption, access control, and user authentication.</p> </li> </ol>","tags":["design"]},{"location":"blog/2023/11/02/masstransit-pt1/#rabbitmq-queues","title":"RabbitMQ Queues","text":"<p>A RabbitMQ message queue functions as a buffer, temporarily storing messages until they are ready for processing by the consuming application or service. Each message within a queue possesses a unique identifier and remains in memory or on disk until it is either consumed by a consumer or reaches its predefined time-to-live (TTL). RabbitMQ queues can be configured to support various message delivery types, such as round-robin, priority-based, or topic-based routing.</p> <p>At a high level, consider a message queue as a sizable buffer capable of retaining all your messages, awaiting consumption by the intended consumers.</p> <p>Below is a diagram showing at a high level how a message queue works. You have one or more producers (the client) generating messages and publishing them to a queue.  One or more consumers (the worker) can then consume the messages in the queue.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant Message Queue\n    participant Worker\n\n    Client-&gt;&gt;Message Queue: Send Message\n    Message Queue-&gt;&gt;Worker: Enqueue Message\n    Worker-&gt;&gt;Message Queue: Dequeue Message\n    Message Queue-&gt;&gt;Worker: Return Message\n    Worker-&gt;&gt;Client: Process Message</code></pre>","tags":["design"]},{"location":"blog/2023/11/02/masstransit-pt1/#rabbitmq-exchanges","title":"RabbitMQ Exchanges","text":"<p>With the basics of queues understood, let's delve further into exchanges. An exchange, a core component of a message broker, receives messages from producers and directs them to message queues based on specific routing rules. Exchanges are responsible for receiving messages and routing them to one or more queues, determined by message type and routing keys.</p> <p>How does this work in practice? When a producer dispatches a message to RabbitMQ, it first reaches an exchange, which subsequently routes the message to one or more message queues based on message type and routing key. MassTransit, by default, employs the Fanout routing algorithm, which is the focus of this tutorial. However, for reference, a list of different exchange types supported by RabbitMQ concerning message routing is provided:</p> <ol> <li>Direct Exchange - Direct exchanges route messages to a queue based on the exact match between the routing key and the binding key.</li> <li>Fanout Exchange - Fanout exchanges route messages to all the queues that are bound to them, regardless of the routing key. (this is the one we will be working with in this tutorial)</li> <li>Topic Exchange - Topic exchanges route messages based on pattern matching between the routing key and the binding key. The routing key can contain wildcards, allowing for more flexible routing.</li> <li>Headers Exchange - Headers exchanges route messages based on the headers of the message, which can be any key-value pair.</li> </ol> <p>The illustration below provides an overview of how the Fanout Exchange operates in RabbitMQ.</p> <pre><code>graph TD\nsubgraph Producer\n    A[Producer] --&gt;|Message| B(Fanout Exchange)\nend\nsubgraph Fanout Exchange\n    B --&gt;|Message| C(Queue 1)\n    B --&gt;|Message| D(Queue 2)\n    B --&gt;|Message| E(Queue 3)\nend\nsubgraph Queue 3\n    E --&gt;|Message| H(Consumer 3)\nend\nsubgraph Queue 2\n    D --&gt;|Message| G(Consumer 2)\nend\nsubgraph Queue 1\n    C --&gt;|Message| F(Consumer 1)\nend</code></pre>","tags":["design"]},{"location":"blog/2023/11/02/masstransit-pt1/#implementation-of-masstransit-in-net-core","title":"Implementation of MassTransit in .NET Core","text":"<p>Now, let's proceed to the part you've eagerly anticipated. If you've read through the introductions, my compliments. I acknowledge it was a substantial amount of information, but it is essential to possess this knowledge to construct a distributed application and grasp the underlying mechanisms.</p>","tags":["design"]},{"location":"blog/2023/11/02/masstransit-pt1/#deploy-rabbitmq-using-docker","title":"Deploy RabbitMQ using Docker","text":"<p>Our initial task is to ensure a running instance of RabbitMQ. I will employ Docker to launch and manage RabbitMQ. You can use PowerShell or your terminal to execute the following command, compatible with both Windows and Linux environments:</p> <pre><code>docker run -d --hostname twc-rabbitmq-server --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:3.13-rc-management\n</code></pre> <p>What transpires when you execute this command?</p> <ul> <li>The most recent available management image of RabbitMQ from Docker is retrieved.</li> <li>The hostname is configured as \"twc-rabbitmq-server,\" which you can customize as desired.</li> <li>The container is named \"rabbitmq.\"</li> <li>Ports 5672 and 15672 are exposed for the container, mapping to the same ports internally. Port 5672 is the designated port for sending and publishing messages, while port 15672 serves as the administration panel accessible via a web browser.</li> <li>With Docker's assistance, RabbitMQ is up and running. To explore the RabbitMQ administration panel, open your web browser and navigate to \"localhost:15672.\" You can log in using the default credentials: \"guest/guest.\" We will delve into the utilization of this web panel later in this article. You may want to quickly check the logs to ensure no errors occurred. You can do so by executing the following command:</li> </ul> <pre><code>docker logs rabbitmq\n</code></pre>","tags":["design"]},{"location":"blog/2025/04/15/mediator/","title":"Evaluating MediatR in Modern .NET (8, 9, 10) Applications","text":"","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#introduction-and-background","title":"Introduction and Background","text":"<p>MediatR is a popular .NET library that implements the Mediator pattern for in-process messaging. It allows decoupling of request senders from their handlers by routing messages (requests/commands/queries) through a central mediator. Over the years it has become a staple in many \u201cClean Architecture\u201d templates and CQRS-style projects, with over 286 million NuGet downloads as of 2025 (MediatR and MassTransit Going Commercial: What This Means For You). MediatR provides a simple request/response API and supports publish/subscribe for in-memory notifications, along with a pipeline behavior feature that enables plugging in cross-cutting concerns (like logging, validation, etc.) around those requests.</p> <p>However, the landscape has changed in recent .NET versions (8 and beyond). The .NET platform now offers native features (such as minimal API endpoint filters, improved middleware, and powerful DI patterns) that cover many use cases MediatR was traditionally used for. Additionally, MediatR\u2019s licensing has evolved \u2013 it is transitioning from a free OSS tool to a commercial (paid) product, raising questions about relying on it for core infrastructure (AutoMapper and MediatR Going Commercial) (MediatR and MassTransit Going Commercial: What This Means For You). This article provides a comprehensive evaluation of using MediatR in modern .NET applications, comparing its benefits and drawbacks to native alternatives, and offering guidance on when (if ever) it remains the right choice.</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#overview-what-mediatr-does","title":"Overview: What MediatR Does","text":"<p>MediatR\u2019s primary role is to reduce direct dependencies between components by providing a central mediator for interactions. Instead of a controller or service directly instantiating or calling a handler class, it sends a request to <code>IMediator</code>, which in turn finds and invokes the appropriate handler. This results in a clean separation: the sender only knows about the request and the mediator, not the concrete handler. MediatR supports: </p> <ul> <li>Request/Response handling: e.g. a <code>CreateOrderCommand</code> is sent and a <code>OrderCreatedResult</code> is returned by the corresponding handler.</li> <li>Notifications (pub/sub): one event can be published via the mediator to multiple in-process notification handlers.</li> <li>Pipeline Behaviors: custom pre- and post-processing logic that wraps the handling of requests (similar to middleware but at the MediatR pipeline level).</li> </ul> <p>Example \u2013 Using MediatR to handle a command with a pipeline behavior: </p> <pre><code>// Define a request (command) and its response\npublic record CreateOrderCommand(OrderDto Order) : IRequest&lt;OrderResult&gt;;\n\n// Implement the handler for the request\npublic class CreateOrderHandler : IRequestHandler&lt;CreateOrderCommand, OrderResult&gt; {\n    public async Task&lt;OrderResult&gt; Handle(CreateOrderCommand request, CancellationToken ct) {\n        // Business logic to create order\n        // ... (e.g., save to DB)\n        return new OrderResult { Success = true, OrderId = /* new ID */ };\n    }\n}\n\n// Define a pipeline behavior (e.g., logging) that wraps all requests\npublic class LoggingBehavior&lt;TRequest, TResponse&gt; : IPipelineBehavior&lt;TRequest, TResponse&gt; {\n    public async Task&lt;TResponse&gt; Handle(TRequest req, RequestHandlerDelegate&lt;TResponse&gt; next, CancellationToken ct) {\n        Console.WriteLine($\"Handling {typeof(TRequest).Name}\");\n        var response = await next();  // call the next piece in pipeline (eventually the handler)\n        Console.WriteLine($\"Handled {typeof(TRequest).Name}\");\n        return response;\n    }\n}\n</code></pre> <p>In an ASP.NET Core app, once MediatR is configured (e.g. <code>services.AddMediatR(...)</code>), any component (like a controller or minimal API endpoint) can send the <code>CreateOrderCommand</code> via <code>await _mediator.Send(command)</code> and receive the result. The registered <code>LoggingBehavior</code> will run automatically before and after the handler, without the controller needing to explicitly call any logging logic.</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#pros-of-using-mediatr-in-process-messaging-benefits","title":"Pros of Using MediatR (In-Process Messaging Benefits)","text":"<p>Using MediatR can provide several advantages in a modern .NET application:</p> <ul> <li> <p>Decoupling and Separation of Concerns: Senders of requests (UI layers or other services) do not need to know implementation details of handlers. This promotes a cleaner architecture where business logic is isolated. It\u2019s especially useful in large systems where many different parts may request the same operations. MediatR \u201cdecouples requests from the objects handling them, promoting separation of concerns\u201d (MediatR and MassTransit Going Commercial: What This Means For You). This decoupling can make it easier to refactor or change implementations without affecting the calling code (         Craig Mattson's Blog :: MediatR vs. Minimal APIs vs. FastEndpoints).</p> </li> <li> <p>Consistent Structure (CQRS-friendly): MediatR encourages a use-case centric design: each request (command/query) is represented by a class and handled by a dedicated handler. This aligns naturally with the Command-Query Responsibility Segregation (CQRS) pattern, where commands and queries are distinct and handled separately (Stop Conflating CQRS and MediatR). Many teams find this one-class-per-use-case approach improves code organization (logic isn\u2019t all lumped into large service classes).</p> </li> <li> <p>Pipeline Behaviors (Middleware for Handlers): The pipeline behavior feature acts like an in-process middleware for requests (MediatR Pipeline Behaviors and Fluent Validation in NET 8 ...). You can write behaviors to handle cross-cutting concerns such as logging, validation, authorization, exception handling, caching, etc., and they will wrap around all (or specific) requests. This avoids duplicating such boilerplate in every handler. For example, one can implement a single validation behavior that automatically validates any request object that has validators associated, before it reaches its handler. This centralized approach is very powerful for consistent policies.</p> </li> <li> <p>In-Process Publish/Subscribe: In addition to request/response, MediatR supports notifications (<code>INotification</code>) that multiple handlers can subscribe to. This is handy for domain events within a monolith: one part of the system can publish an event (e.g. <code>OrderCreatedEvent</code>) and multiple other parts can react to it (e.g., send email, update inventory, etc.), all in-memory. It provides a simple event aggregator pattern without setting up external message brokers.</p> </li> <li> <p>Thin Controllers/Endpoints: By using MediatR, you can keep your controllers or API endpoints extremely thin. They often just translate an HTTP request to a MediatR call. Rather than injecting many services into a controller, you inject a single <code>IMediator</code>. This means the controller doesn\u2019t need to know about all the dependencies required to handle a request \u2013 it simply hands off to the mediator. This can make the API layer very streamlined and focused purely on request/response mapping.</p> </li> <li> <p>Testability: The mediator pattern can aid testing since handlers can be unit-tested in isolation (they are just classes implementing an interface, with their dependencies injected). The <code>IMediator</code> itself can be mocked if needed to test higher layers. Also, because of decoupling, you can swap out implementations or run handlers in isolation more easily.</p> </li> <li> <p>Mature Ecosystem: MediatR has been around for many years with a stable API. There is plentiful documentation, examples, and extension libraries (for example, integration with FluentValidation via pipeline, etc.). Senior .NET developers likely have experience with it, and it\u2019s battle-tested in many production systems.</p> </li> </ul> <p>In summary, MediatR can bring a clean, pattern-based approach to structuring business logic and handling cross-cutting concerns in-process. It shines in complex applications where decoupling internals improves maintainability or when implementing patterns like CQRS.</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#cons-and-pitfalls-of-using-mediatr","title":"Cons and Pitfalls of Using MediatR","text":"<p>Despite its benefits, MediatR introduces additional abstraction that may not always be justified. Some common drawbacks and criticisms include:</p> <ul> <li> <p>Indirection and Complexity: MediatR adds an extra layer of indirection to calls. Instead of a straightforward method call from one class to another, you send a request through the mediator, which magically invokes the right handler. This can make the code harder to trace and reason about, especially for newcomers to the project (Stop Conflating CQRS and MediatR). The flow of logic is not explicit in the code; developers may need to jump between the request definition and its handler to understand what happens. Overusing MediatR (e.g. for every single small operation) can result in a codebase where it\u2019s non-obvious how things tie together at runtime. As one author noted, \u201cThe indirection MediatR introduces is its most criticized aspect. It can make code harder to follow, especially for newcomers to the codebase.\u201d (Stop Conflating CQRS and MediatR) </p> </li> <li> <p>Boilerplate and Proliferation of Types: A MediatR-centric design tends to produce a lot of small classes: one for each request (command/query), one for each response (often), one handler per request, plus any number of behavior classes, notification classes, etc. While each piece is individually simple, the total number of files and types can grow large, which adds overhead in navigating and maintaining the code. In many cases, the amount of code ends up being similar to just writing services and methods directly (         Craig Mattson's Blog :: MediatR vs. Minimal APIs vs. FastEndpoints) \u2013 just split differently. Craig Mattson notes that you often \u201cend up writing the same amount of code anyway \u2013 and you don\u2019t get the same level of [clarity]\u201d when using MediatR with lots of handlers and interfaces (         Craig Mattson's Blog :: MediatR vs. Minimal APIs vs. FastEndpoints).</p> </li> <li> <p>Performance Overhead (Minor but Present): MediatR is essentially an in-memory dispatch, which is quite fast (it mostly uses reflection or compiled expressions internally to locate and invoke handlers). For most applications the overhead is negligible (microseconds per call). However, in extremely high-throughput scenarios or hot code paths, this extra hop and resolution might have a cumulative impact. Large enterprise applications with tens of thousands of operations per second might prefer to eliminate even small overheads. The indirection also makes it a bit harder to optimize or use advanced features like source generation (some newer mediator libraries use source generators to minimize overhead, whereas MediatR\u2019s base version does runtime scanning). Overall, performance is usually not a major concern for typical usage, but it\u2019s a consideration for very performance-sensitive code.</p> </li> <li> <p>Hidden Dependencies (Implicit Wiring): While having thin controllers is nice, it also means the controller\u2019s dependencies are \u201chidden\u201d behind the mediator. This can be viewed as a Service Locator pattern in disguise \u2013 the mediator, via the DI container, finds and injects the actual services needed by the handler at runtime. Some consider service locator an antipattern because it obscures what components are being used (You Probably Don't Need to Worry About MediatR). For example, if a handler requires IRepository, an email sender, etc., the controller does not list those, so at the API level it's not obvious what the operation actually involves. This can complicate understanding the system\u2019s true dependencies. (Jimmy Bogard has argued MediatR is not literally a service locator because it doesn\u2019t arbitrarily resolve services, only handlers (You Probably Don't Need to Worry About MediatR), but the effect of indirection is similar in practice to some.)</p> </li> <li> <p>Lack of Explicitness: In a similar vein, using MediatR means that the explicit method call hierarchy is lost. Instead of seeing in code \u201cController calls ServiceA, which calls RepositoryB\u201d, you see \u201cController sends XRequest\u201d. The actual call chain is determined by convention (which handler implements XRequest) rather than an explicit reference. This can violate the \u201cExplicit Dependencies Principle\u201d according to some purists (You Probably Don't Need to Worry About MediatR). It\u2019s a trade-off: you gain flexibility and decoupling at the cost of clarity in the flow. Teams must decide if that trade-off is worthwhile. Well-established teams may mitigate this by naming conventions and documentation, but new team members might still face a learning curve.</p> </li> <li> <p>Overuse and Misuse: It is easy to adopt MediatR everywhere (\u201cwhen you have a hammer, everything looks like a nail\u201d). Not every project or every component benefits from the mediator pattern. For simple CRUD applications or small codebases, adding this pattern can be over-engineering. If every controller call simply wraps a single handler with no real decoupling needed, MediatR might be an unnecessary abstraction. In some cases, developers have even tried to replace a traditional service layer entirely with MediatR, which can lead to a fragmented design where business logic is spread across dozens of handlers with no unifying interface. Overuse can also make debugging harder, as you have to inspect each handler class individually for logic.</p> </li> <li> <p>Custom Abstraction Pitfalls: Some teams, in an attempt to keep their core \u201cclean,\u201d will create their own abstraction on top of MediatR (for example, an <code>IMessageBus</code> interface that internally calls MediatR). This often adds little value and can complicate maintenance. Creating a custom mediator abstraction means you now have to maintain that wrapper, and you might lose some capabilities of MediatR or make updates harder. Since MediatR is a well-known interface already (<code>IMediator</code>), adding another layer tends to be redundant \u2013 you could instead directly reference MediatR or, if you want to avoid locking into it, consider not using it at all. In short, over-abstracting the mediator pattern (writing your own mediators or wrapper interfaces everywhere) can lead to \u201cunnecessary complexity\u201d without much benefit (Don't use MediatR by default on all .NET projects). The GoatReview article \u201cDon\u2019t use MediatR by default\u201d demonstrates how one can achieve a use-case-driven architecture without the library, suggesting that blindly inserting an extra messaging layer is often needless (Don't use MediatR by default on all .NET projects) (         Craig Mattson's Blog :: MediatR vs. Minimal APIs vs. FastEndpoints).</p> </li> <li> <p>License and Dependency Risk: A new development as of 2023-2025 is that MediatR is no longer guaranteed to be free for future versions. The author announced he will be \u201ccommercializing MediatR\u201d to ensure sustainability (AutoMapper and MediatR Going Commercial). This means upcoming major releases (post v12) are likely to require a paid license for use in businesses. Relying on a third-party for core application flow always carried some risk (if the maintainer stopped working on it, for example), but now there\u2019s a tangible consideration of cost and license compliance. Using MediatR in your architecture means you either: stick with the last OSS version (missing out on future improvements), or budget for license fees (if you want the latest updates/support) (MediatR and MassTransit Going Commercial: What This Means For You). Some organizations may be averse to introducing paid dependencies at the heart of their system. It also serves as a reminder that any critical third-party library can change licensing or support terms, which is a risk factor for long-term projects. In contrast, leaning on built-in .NET framework features (which are supported by Microsoft and come with the platform) might offer more stability and no additional cost.</p> </li> </ul> <p>To summarize, MediatR should be used judiciously. As one article title succinctly put it: \u201cDon\u2019t use MediatR by default on all .NET projects\u201d (Don't use MediatR by default on all .NET projects). Its usage should be justified by clear benefits in architecture or maintainability. Otherwise, modern .NET offers simpler options that might serve you better, as we discuss next.</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#mediatr-vs-native-net-features-endpoint-filters-middleware-di","title":"MediatR vs. Native .NET Features (Endpoint Filters, Middleware, DI)","text":"<p>With .NET 6, 7, and now 8, the framework itself has evolved features that overlap with MediatR\u2019s functionality for handling cross-cutting concerns and achieving separation. Senior .NET developers should consider these native approaches, as they often reduce external dependencies and can simplify the design. Here we compare MediatR\u2019s model (request/handler + pipeline) with native .NET constructs:</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#endpoint-filters-minimal-apis-vs-mediatr-pipeline-behaviors","title":"Endpoint Filters (Minimal APIs) vs. MediatR Pipeline Behaviors","text":"<p>.NET 7 introduced Endpoint Filters for minimal APIs, and .NET 8 refined them further. Endpoint filters allow you to run code before and/or after the execution of a minimal API handler, in a way very similar to MediatR\u2019s pipeline behaviors. They effectively act as middleware at the endpoint level.</p> <p>For example, suppose we want to log and validate a request to create an order. Using MediatR, we might write a pipeline behavior for logging, and perhaps another for validation, that wrap the <code>CreateOrderCommand</code> handler. With minimal APIs and endpoint filters, we can achieve the same without a mediator:</p> <pre><code>// Define an order creation service for business logic\npublic interface IOrderService { Task&lt;OrderResult&gt; CreateOrderAsync(OrderDto dto); }\n\nvar app = WebApplication.CreateBuilder(args).Build();\n\n// Minimal API endpoint with an endpoint filter for cross-cutting concerns\napp.MapPost(\"/orders\", async (OrderDto order, IOrderService svc) =&gt; {\n        // Directly call the service to perform the action\n        OrderResult result = await svc.CreateOrderAsync(order);\n        return Results.Ok(result);\n    })\n   .AddEndpointFilter(async (context, next) =&gt; {\n        // This code runs before the main handler\n        var dto = context.GetArgument&lt;OrderDto&gt;(0);\n        if (dto == null) return Results.BadRequest(\"Order data is required\");\n        Console.WriteLine($\"[Request] Creating order {dto.Id}...\");\n        var result = await next(context);  // call the inner handler (the lambda above)\n        // This code runs after the handler\n        Console.WriteLine($\"[Response] Order creation completed.\");\n        return result;\n   });\n\napp.Run();\n</code></pre> <p>In the above snippet, the endpoint filter does logging (and a null-check) before calling the <code>IOrderService</code>, and logs after. This is analogous to a MediatR LoggingBehavior. We didn\u2019t need a mediator to achieve this separation \u2013 the filter cleanly encapsulates the cross-cutting logic. Endpoint filters can be reused or applied globally as needed, and multiple filters can be chained (just like multiple pipeline behaviors). </p> <p>Advantages of Endpoint Filters: They are built-in, lightweight, and targeted. They integrate naturally with minimal APIs (which are increasingly favored in .NET 6+ for simple services). One LinkedIn discussion even noted that with minimal APIs plus endpoint filters, \u201cMediatR is not necessary anymore\u2026 endpoint filters [do] pretty [much the] same thing with one less package dependency.\u201d (Probably the best pattern that involves MediatR, FluentValidation &amp;\u2026 | Mukesh Murugan) This highlights that the platform itself is catching up to what MediatR offered, at least in terms of wrapping logic around handlers.</p> <p>If your project uses MVC controllers instead of minimal APIs, you have analogous features: you can use Action Filters or Result Filters (attributes or middleware) to achieve similar effects for cross-cutting concerns on controllers/actions. (In fact, MediatR pipeline behaviors were partly inspired by the concept of ASP.NET MVC filters, but applying them at the business logic layer). The difference with endpoint filters is they work with the new minimal API model and are very flexible (you can even inject services into filters or have them globally).</p> <p>When to prefer filters over MediatR pipeline: In modern .NET 8 projects that use minimal APIs, endpoint filters provide a straightforward way to add cross-cutting behavior. They keep the code localized (near the endpoint definition) and avoid the need for extra libraries. Use them if you want to keep your stack minimal and leverage built-in capabilities. Filters are especially suitable when the cross-cutting logic is primarily about handling HTTP concerns (validation of request models, transforming responses, etc.), as opposed to deep business logic policies.</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#aspnet-core-middleware-vs-mediatr-behaviors","title":"ASP.NET Core Middleware vs. MediatR Behaviors","text":"<p>ASP.NET Core\u2019s request middleware pipeline (the <code>app.Use...</code> components that run for every HTTP request) is another place to handle cross-cutting concerns. Middleware operates at the HTTP request/response level, before it even reaches your API endpoint or controller. Common tasks like authentication, error handling, logging, and compression are typically done with middleware.</p> <p>Many concerns that one might implement via MediatR behaviors could also be done in middleware. For example:  - Logging: Instead of a MediatR logging behavior that logs each request handled by the mediator, you might log each incoming HTTP request (and outgoing response) via a middleware. This gives a high-level audit of all requests. - Exception Handling: It\u2019s common to have a global exception handling middleware that catches unhandled exceptions and transforms them into an HTTP response (500 or a formatted error). This can replace the need for a MediatR behavior that wraps every request handler in a try-catch. - Performance timing, caching: These can often be handled by middleware or response caching mechanisms at the ASP.NET level.</p> <p>The key difference is scope and granularity: Middleware sees the entire HTTP transaction but doesn\u2019t inherently know about the specific MediatR request inside (if any). MediatR behaviors, on the other hand, see the specific request object and response at the business logic level. In practice, many cross-cutting concerns don\u2019t need that specificity. For example, logging the fact that \u201cOrder X created\u201d can be done in the handler itself or via a domain event, rather than at the mediator pipeline. Other things, like validating an incoming JSON payload, are often done via model binding or filters (or FluentValidation integration) rather than a MediatR pipeline.</p> <p>Use middleware for: - Global policies that apply to all requests uniformly (auth, error handling, global logging, etc.). - Scenarios where the concern is not tied to a specific operation but to the request or response as a whole (e.g., adding a header to every response).</p> <p>Limitations of middleware relative to MediatR: - Middleware doesn\u2019t easily allow skipping or altering the execution of individual handlers beyond the HTTP level. MediatR behaviors can decide not to call the next handler based on business logic (though that\u2019s rare; usually one would throw or return an error result). - Middleware can\u2019t differentiate between different MediatR request types within the HTTP flow (it only sees the URL/path or other HTTP info). Pipeline behaviors know the actual request type class, which can be useful for very targeted behavior (e.g., \u201cfor all <code>Create</code> commands do X\u201d).</p> <p>In modern .NET, one might use a combination: rely on middleware for broad concerns and filters or code in handlers for specific ones. This often leaves little need for a dedicated MediatR pipeline layer.</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#direct-service-calls-dependency-injection-vs-mediatr-requests","title":"Direct Service Calls &amp; Dependency Injection vs. MediatR Requests","text":"<p>One of MediatR\u2019s selling points is that you can request an operation without directly referencing the implementing class. But .NET\u2019s built-in Dependency Injection (DI) already allows decoupling via interfaces. Rather than using a mediator to call a handler, you could define an interface for that functionality and inject it where needed. This is the classic approach of layering an application (Controller -&gt; Service -&gt; Repository, etc.). For example, instead of a <code>CreateOrderCommand</code> + handler, you might have an <code>IOrderService</code> with a method <code>CreateOrder(dto)</code> that the controller calls. The service can implement any needed logic, call repositories, publish events, etc., without a mediator in the middle.</p> <p>Direct method calls and DI have some advantages in clarity: - It\u2019s immediately obvious what gets called. If a controller calls <code>orderService.CreateOrder()</code>, you can navigate to that method definition directly. - Fewer moving parts: no pipeline, no request/response classes (you likely just pass a DTO or parameters). - Less ceremony for simple operations.</p> <p>You can still achieve separation by interface. In a way, an <code>IOrderService</code> is itself an abstraction that decouples the controller from the implementation (much like a mediator decouples it from a handler). The difference is it\u2019s one-to-one coupling (controller knows it\u2019s calling an order service) versus mediated coupling (controller only knows mediator, which then decides who to call).</p> <p>Orchestration logic: In more complex workflows, you might have one action trigger multiple operations. With MediatR, one might send a command and then publish notifications for other parts to react. With a service-based approach, the service could orchestrate those calls explicitly. For instance, <code>OrderService.CreateOrder</code> could internally call <code>InventoryService.UpdateStock</code> and <code>EmailService.SendConfirmation</code> directly (via DI). This is a straightforward procedural approach. The downside is the OrderService now needs references to those other services (increasing coupling), whereas with MediatR notifications, OrderService could just publish an <code>OrderCreatedEvent</code> and not care who handles it. That said, using events within the same process can also be done without MediatR \u2013 one can implement a simple in-memory event dispatcher or use C# events or another library (or even something like the Wolverine library for in-process messaging). </p> <p>DI patterns for cross-cutting: Another point is that using pure DI, you can implement the decorator pattern or interceptors for cross-cutting concerns. For example, you could have <code>IOrderService</code> decorated by a <code>LoggingOrderService : IOrderService</code> that logs calls and then delegates to the real implementation. .NET\u2019s DI container (with help from libraries like Scrutor) can wire this up so that whenever <code>IOrderService</code> is resolved, you actually get the logging decorator which wraps the real service. This achieves a similar effect to a pipeline behavior, though setting up many decorators can become cumbersome (and it\u2019s not as generic as MediatR\u2019s pipeline, which covers all requests in one go). </p> <p>In summary, leveraging well-designed service classes and DI can cover 80-90% of what you might use MediatR for, especially in .NET 8/9 solutions. You organize your code by functionality (services, repositories, etc.), use filters/middleware for cross-cutting policies, and you get a simpler call flow. The GoatReview article demonstrates a side-by-side of implementing a use-case with and without MediatR \u2013 ultimately showing that a carefully crafted \u201cuse-case-centric architecture without relying on MediatR\u2019s internal messaging\u201d is feasible and can be cleaner for many projects (Don't use MediatR by default on all .NET projects).</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#summary-of-native-alternatives","title":"Summary of Native Alternatives","text":"<ul> <li>Endpoint Filters (in minimal API): Replace many uses of pipeline behaviors for pre/post logic, especially in .NET \u215e minimal API apps (Probably the best pattern that involves MediatR, FluentValidation &amp;\u2026 | Mukesh Murugan). They reduce the need for an in-process mediator for the sake of cross-cutting concerns.</li> <li>ASP.NET Middleware / Filters: Handle global or coarse-grained cross-cutting concerns (auth, error handling, etc.) without any mediator. Leverage the framework\u2019s pipeline.</li> <li>Direct Calls via DI: Call services or handlers directly through interfaces. Use traditional OOP patterns (interfaces, inheritance, events) to achieve decoupling rather than a mediator library. This often results in more straightforward code for simple scenarios.</li> <li>Other Libraries: If a mediator pattern is still desired but you don\u2019t want MediatR specifically (maybe due to licensing or overhead), there are alternatives. For instance, FastEndpoints (an open-source library) provides a minimal API + CQRS framework that can eliminate the need for controllers and MediatR by combining the concepts. Other lightweight mediator implementations or source-generated dispatchers exist (e.g., Martin Thamar\u2019s Mediator or Wolverine for in-process and out-of-process messaging). Before adopting another library, though, evaluate if the native solutions suffice.</li> </ul>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#the-cost-of-custom-abstractions-avoiding-over-engineering","title":"The Cost of Custom Abstractions (Avoiding Over-Engineering)","text":"<p>A specific pitfall to be wary of is creating custom abstraction layers around MediatR or similar patterns. Some architects attempt to future-proof their design by not referencing MediatR directly in their application core \u2013 for example, defining an interface <code>IApplicationBus</code> with methods like <code>Send()</code> and having an infrastructure implementation that calls MediatR. The intention is to be able to swap out MediatR if needed, or to make the core completely ignorant of external libraries.</p> <p>In practice, this often introduces more complexity than it removes: - You end up writing a thin wrapper that likely has a 1:1 mapping to MediatR\u2019s own interface. This is essentially duplicate code that needs to be maintained and understood. - If MediatR changes (especially now moving to commercial license), your wrapper might shield some impact but you still have to update the implementation. And if you decide to remove MediatR, you\u2019ll now have to implement the mediator behavior yourself behind that interface anyway. - It can confuse developers: \u201cDo I use IMediator or IApplicationBus? What\u2019s the difference?\u201d Unless very well documented, such abstractions can be unclear. - MediatR is already an abstraction (the mediator pattern) \u2013 wrapping an abstraction with another abstraction can lead to indirection hell. You might find yourself debugging through your wrapper only to hit the next layer (the actual MediatR call). </p> <p>Custom mediator implementations: Another angle is writing your own mediator from scratch (instead of using MediatR). If your needs are simple, this is actually not too hard \u2013 e.g., one can write a service that maintains a dictionary of request types to handler functions and uses DI to resolve and invoke them. This avoids an external dependency and could be tailored to your exact requirements. The downside is you must implement (and test) features that MediatR already provides (caching of handler lookup, supporting multiple handlers for notifications, possibly a pipeline mechanism). It\u2019s doable but you should only undertake this if you have a very good reason (such as avoiding the new license cost, or needing a very specific behavior). Milan Jovanovi\u0107 suggests \u201cMediatR isn\u2019t too complex to build on your own\u201d and even recommends it as an exercise (MediatR and MassTransit Going Commercial: What This Means For You) \u2013 but keep in mind maintenance burden. A custom solution becomes your responsibility entirely.</p> <p>Recommendation: Don\u2019t add an abstraction layer just for the sake of it. If you decide to use MediatR, feel free to use it directly in your application code (it\u2019s okay for your Application layer to depend on it if it\u2019s a conscious choice). If you decide not to use it, then call your services or handlers directly. The middle ground of writing your own mediator abstraction should be reserved for cases where you have very unique requirements or are extremely concerned about vendor lock-in. In most cases, it\u2019s better to either commit to the library or not use it at all, rather than adding needless indirection.</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#mediatrs-new-licensing-weighing-third-party-dependencies","title":"MediatR\u2019s New Licensing \u2013 Weighing Third-Party Dependencies","text":"<p>It is important to highlight that MediatR (along with some other .NET staples like AutoMapper and MassTransit) is transitioning to a commercial license model as of 2025. The creator, Jimmy Bogard, announced this move to ensure he can sustainably maintain the library (AutoMapper and MediatR Going Commercial). Concretely, this means future versions (beyond the currently available ones) will likely require purchasing a license for use in projects (exact pricing and terms to be determined as of the announcement). Current open-source versions remain available (and Jimmy has indicated security patches will continue for a while), but new features or official support will be behind a commercial agreement (MediatR and MassTransit Going Commercial: What This Means For You).</p> <p>Implications for your team: - Cost: If you plan to keep up-to-date with MediatR in .NET 8/9/10, you may need to allocate budget for it. For a large team or organization, this might be a minor expense relative to project budgets, but it must be justified (especially if similar functionality is achievable without cost). - Version Lock-in: If you choose not to pay, you might end up stuck on the last free version. This could be fine for now \u2013 MediatR is mature \u2013 but over a span of years, .NET 10+ might introduce changes that necessitate library updates. There\u2019s a risk that using an old version eventually becomes a maintenance problem (if it doesn\u2019t support a new .NET runtime feature or has bugs that won\u2019t be fixed in OSS version). - Third-Party Risk: This situation underscores the general risk of relying on third-party libraries for critical architecture. If the library becomes paid or is discontinued, you face a tough choice: adapt your code or pay up. Some developers on forums have reacted by saying it \u201cbecomes too risky, both financially and strategically, to rely on open-source third-party libraries at all\u201d for core functionality (.NET library MassTransit going commercial with V9 | Hacker News) (MediatR is going commercial #1105 - GitHub). That\u2019s an extreme view, but it highlights a preference for using platform capabilities over external libraries when possible.</p> <ul> <li>Community and Ecosystem: It\u2019s possible that fewer new projects will adopt MediatR by default once it\u2019s commercial (developers might choose alternatives or simpler designs to avoid the hassle). If community usage diminishes, community support (blogs, Q&amp;A) might also slow down over time. On the other hand, a paid model might lead to a more professionally supported product if you do buy in. Your team should gauge whether having a support channel and active development (through a paid license) is valuable, or if using the free built-in approaches is more aligned with your strategy.</li> </ul> <p>In short, the licensing change doesn\u2019t immediately break anything \u2013 you can continue using MediatR in .NET 8 as is. But it is a strategic inflection point to consider: do we want our core application flow tied to a now-commercial tool, or can we achieve our goals with the rich toolset provided by .NET itself? Many teams are taking this opportunity to reevaluate the necessity of MediatR in new projects.</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#when-might-mediatr-still-be-appropriate","title":"When Might MediatR Still Be Appropriate?","text":"<p>Given all the above, are there scenarios where using MediatR (even in .NET 8/9/10 era) is still a valid or optimal choice? The answer is \u201cyes, in certain cases\u201d \u2013 although those cases are narrower than before. Consider using (or continuing with) MediatR in situations like:</p> <ul> <li> <p>Complex, Modular Monoliths: If you have a large monolithic application split into distinct modules or bounded contexts, in-process messaging can decouple those modules. MediatR\u2019s notification (pub-sub) model can allow one module to react to events from another without tight coupling. For example, in a modular monolith architecture, you might not want the Orders module to directly reference the Notifications module \u2013 using MediatR to publish an <code>OrderPlacedEvent</code> that the Notifications module handles keeps boundaries clear. While this can be implemented without MediatR, the library makes it trivial to set up. In such cases, the benefit of decoupling might outweigh the added complexity.</p> </li> <li> <p>Extensive Cross-Cutting Requirements: If your application has a lot of sophisticated cross-cutting concerns that must be applied to many or all operations (e.g., detailed auditing, multi-step validation, transaction scoping, etc.) and you prefer to implement these in the mediator pipeline, MediatR provides a ready framework. For instance, pipeline behaviors that enforce business rules before every command can ensure consistency. Could you do it with filters or AOP? Possibly, but MediatR might result in a cleaner, more domain-specific implementation (since behaviors operate in the context of application requests, not just raw HTTP). If you already have a suite of behaviors (like logging, validation, authorization checks) working in MediatR, porting all that to another approach might not be worth it.</p> </li> <li> <p>Team Familiarity and Existing Investment: If your development team is already highly familiar with MediatR and perhaps your codebase is built around it, there\u2019s a non-trivial cost to rip it out. Provided you\u2019re satisfied with its performance and you have a plan for the licensing (e.g., stick to current version or are willing to pay for an upgrade when needed), it might be perfectly reasonable to continue using it. There\u2019s nothing inherently \u201cbroken\u201d about MediatR on .NET 8 \u2013 it still works as it always has. So long as the team finds it valuable and it\u2019s not causing maintenance headaches, you might choose to keep it, especially in an existing project. Just make sure everyone agrees on the rationale (for example: \u201cWe use MediatR because we rely on in-process events heavily and it saves us a lot of boilerplate\u201d).</p> </li> <li> <p>In-Process Messaging as a Design Choice: Some architectures treat in-process messaging as a first-class concept \u2013 almost like having a message bus inside the app. If you foresee potentially moving to a distributed system or want to design your app in a message-driven way, using MediatR could be a stepping stone. It enforces a certain discipline (even if everything is synchronous in-memory now, you think of interactions as messages). Later, you might replace the backend of the mediator with an actual messaging infrastructure. There are libraries (like MassTransit or NServiceBus) that can bridge in-process and out-of-process messaging, but MediatR itself is purely in-memory. Nonetheless, for some complex enterprise scenarios, having the mediator pattern in place can ease future transitions (with the caveat that MediatR itself won\u2019t scale out, but it shapes your code in a way that switching to a real queue is easier). If this forward-looking design is a goal, you might still opt to start with MediatR for its convenient API and then gradually evolve.</p> </li> <li> <p>Multiple UI Entry Points or Background Processing Needs: If the same business logic needs to be invoked from different entry points (say, an API, and also a background job, and also maybe a gRPC service), having that logic encapsulated in a MediatR handler can be handy. All those callers can just send the same request to the mediator. This avoids duplicating orchestration logic across different controllers or services. (Note: This can also be solved by calling a service method directly \u2013 e.g., both your controller and your Hangfire job could call <code>OrderService.CreateOrder</code> \u2013 so MediatR is not the only way. But some prefer the symmetry of using requests for this.)</p> </li> </ul> <p>In essence, MediatR might still be appropriate when its specific strengths line up with your requirements \u2013 i.e., when decoupling and in-process messaging are key architectural drivers, and the team is equipped to manage the abstraction. It\u2019s less appropriate if you\u2019re using it just because \u201cthat\u2019s what the template did\u201d or to achieve things that the framework now provides out-of-the-box.</p> <p>Caution: Even if you identify with some of the above scenarios, consider scope. Perhaps only a part of your system truly benefits from MediatR (e.g., the domain event handling part), whereas your simple CRUD endpoints might not. You can adopt MediatR in a targeted manner \u2013 it\u2019s not all-or-nothing. For example, you might decide to use MediatR for domain events and notifications in your core domain, but use regular controller-&gt;service calls for application commands. It\u2019s perfectly fine to mix approaches as long as it\u2019s kept understandable.</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#decision-matrix-mediatr-vs-native-approaches","title":"Decision Matrix: MediatR vs. Native Approaches","text":"<p>To assist in decision-making, below is a summary matrix of scenarios and the recommended approach in modern .NET:</p> Scenario / Requirement Recommended Approach Simple CRUD or small-scale application \u2013 Few cross-cutting policies, straightforward logic. Avoid MediatR. Use controllers or minimal APIs calling services/repositories directly. Leverage model binding and validation attributes for input checking. MediatR would add unnecessary indirection. Need for Cross-Cutting Concerns (logging, validation, etc. on all requests) without tying to MediatR. Use native pipeline: Endpoint Filters (for minimal API) or MVC Filters/Middleware. These can handle concerns globally or per endpoint, no third-party needed ([Probably the best pattern that involves MediatR, FluentValidation &amp;\u2026 Complex business workflows that require orchestrating multiple operations in sequence. Use service-layer orchestration. Create application services that call the needed components in order. If decoupling is needed, use domain events or callbacks. MediatR can be used, but often explicit calls are easier to follow in this case. In-process Pub/Sub for Domain Events within a rich domain model (especially in a modular monolith). Consider MediatR\u2019s notification feature. This is one area where MediatR shines \u2013 it\u2019s simple and reliable for in-memory events. Alternatively, implement a lightweight domain events dispatcher yourself. If many modules or team boundaries are involved, MediatR can reduce coupling. High sensitivity to performance (e.g., low-latency games, high-frequency trading apps, etc.). Avoid MediatR. The overhead is small, but in extreme cases even small allocations or delays matter. Writing direct calls or optimized patterns (including source-generated code) would be preferable. Existing project heavily invested in MediatR (lots of handlers, behaviors, etc.). Continue with MediatR (for now). There is no urgent need to remove it if it\u2019s working. Plan for the future by monitoring licensing changes: you might stay on the last OSS version or allocate budget for an upgrade. Meanwhile, evaluate if new parts of the system could use simpler patterns to reduce over-reliance on the mediator. Greenfield (new) project in .NET 8/9 with experienced team, aiming for long-term maintainability and minimal external deps. Lean toward native features first. Design the API with filters/middleware and DI. Only introduce MediatR if a clear use case emerges (like needing in-memory pub-sub or avoiding injecting dozens of dependencies into controllers). Often, you\u2019ll find you don\u2019t need it. Requirement for first-class support / SLAs for the mediator component (enterprise support needed). Possibly use MediatR with a commercial license, as it might come with support. Or look at other supported products. If internal SLAs are a concern, relying on built-in .NET is safest since it\u2019s Microsoft-supported. <p>This matrix is a guide \u2013 the final decision should account for the specifics of your system and team. The key is to balance clarity vs. complexity and dependency vs. built-in at the core of your application.</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2025/04/15/mediator/#conclusion-and-recommendations","title":"Conclusion and Recommendations","text":"<p>MediatR has been a valuable tool in the .NET ecosystem, bringing the mediator design pattern to countless projects and helping developers build cleaner code architectures. In the context of .NET 8, 9, and beyond, its role should be re-considered in light of the powerful features now available in the framework itself and the shift towards minimal APIs and simpler pipelines.</p> <p>For a senior .NET development team, the recommendation is to prioritize clarity and native capabilities. Use MediatR only if it demonstrably improves your solution\u2019s design or maintainability. Often, you can achieve the same goals with built-in constructs: - Use minimal API endpoint filters or MVC filters for cross-cutting concerns instead of immediately reaching for pipeline behaviors (Probably the best pattern that involves MediatR, FluentValidation &amp;\u2026 | Mukesh Murugan). - Use well-defined services and DI to call your business logic directly, keeping the flow explicit. - Reserve in-process messaging (like MediatR notifications) for scenarios where decoupling of components is a must-have, not just a nice-to-have.</p> <p>Be mindful of the additional overhead \u2013 both in complexity and now in potential cost \u2013 that adding MediatR brings. Introducing any third-party into your core infrastructure is a strategic decision; with MediatR moving to a paid model, ensure the value it provides is worth that commitment (AutoMapper and MediatR Going Commercial). If your team is comfortable with MediatR and leveraging it in ways that truly benefit the project (for example, implementing a robust CQRS pattern with clear separation), it can still be a worthwhile tool. Just avoid reflexive use \u201cbecause we always did it that way.\u201d</p> <p>Strategic insight: The trend in modern .NET is toward minimalism and performance \u2013 removing unnecessary layers and using source generation or compile-time features to avoid runtime overhead. We see hints that the .NET team is aware of the need for better eventing/mediator patterns in the framework (there have been discussions about a built-in eventing framework in .NET 9/10 (Epic: Eventing Framework in .NET 9 \u00b7 Issue #53219 \u00b7 dotnet/aspnetcore \u00b7 GitHub) (Epic: Eventing Framework in .NET 9 \u00b7 Issue #53219 \u00b7 dotnet/aspnetcore \u00b7 GitHub)), though nothing concrete is arriving in .NET 10 as of now (Epic: Eventing Framework in .NET 9 \u00b7 Issue #53219 \u00b7 dotnet/aspnetcore \u00b7 GitHub). This means the platform might eventually cover MediatR-like scenarios in an official way. Until then, weigh the pros and cons carefully. </p> <p>In conclusion, MediatR is no longer an automatic \u201cyes\u201d for modern .NET apps. It is one of many tools \u2013 use it when it clearly aligns with your needs (and you\u2019re okay with its new licensing), but otherwise favor the simplicity and transparency of native .NET 8+ features. A balanced, case-by-case approach will yield the best architecture: keep things simple by default, and introduce the mediator pattern only to solve specific problems that genuinely require it. </p> <p>References: The analysis above draws on community discussions and expert insights. For instance, developers have noted that minimal APIs with endpoint filters can achieve the same goals as MediatR with fewer dependencies (Probably the best pattern that involves MediatR, FluentValidation &amp;\u2026 | Mukesh Murugan), and that MediatR\u2019s indirection can make code harder to follow in large teams (Stop Conflating CQRS and MediatR). The decision to commercialize MediatR (AutoMapper and MediatR Going Commercial) has further prompted teams to re-evaluate using a third-party mediator. By understanding these factors, a senior team can make an informed decision aligning with both modern .NET capabilities and the project\u2019s long-term interests.</p>","tags":["dotnet","csharp","mediator","clean architecture"]},{"location":"blog/2023/11/06/parsers/","title":"Modeller DSL Parser: Your Gateway to Flexible Parsing in C","text":"<p>Welcome to the world of parsing with Modeller DSL. This lightweight, high-speed, and adaptable parsing library for C# empowers you to construct parsers with ease.</p>","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#getting-started-with-modeller-dsl-parser","title":"Getting Started with Modeller DSL Parser","text":"<p>Modeller.DslParser is a parser combinator library, offering a high-level, declarative approach to crafting parsers. These parsers resemble a high-level specification of a language's grammar, yet they are expressed within a general-purpose programming language, requiring no special tools for producing executable code. Parser combinators offer enhanced capabilities compared to regular expressions, as they can handle a broader range of languages, all while being simpler and more user-friendly than parser generators like ANTLR.</p> <p>At the core of Modeller.DslParser lies the <code>Parser&lt;TToken, T&gt;</code>. It embodies a procedure that consumes a stream of <code>TToken</code>s as input, potentially failing with a parsing error or yielding a <code>T</code> as output. You can conceptualise it as follows:</p> <pre><code>delegate T? Parser&lt;TToken, T&gt;(IEnumerator&lt;TToken&gt; input);\n</code></pre> <p>To embark on your journey of building parsers, you need to import two vital classes that house factory methods: <code>Parser</code> and <code>Parser&lt;TToken&gt;</code>.</p> <pre><code>using Modeller.DslParser;\nusing static Modeller.DslParser.Parser;\nusing static Modeller.DslParser.Parser&lt;char&gt;;\n</code></pre>","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#exploring-primitive-parsers","title":"Exploring Primitive Parsers","text":"<p>Now, let's create some elementary parsers. The <code>Any</code> parser consumes a single character and returns that character.</p> <pre><code>Assert.AreEqual('a', Any.ParseOrThrow(\"a\"));\nAssert.AreEqual('b', Any.ParseOrThrow(\"b\"));\n</code></pre> <p><code>Char</code>, also known as <code>Token</code>, consumes a specific character and returns it, failing if it encounters any other character.</p> <pre><code>Parser&lt;char, char&gt; parser = Char('a');\nAssert.AreEqual('a', parser.ParseOrThrow(\"a\"));\nAssert.Throws&lt;ParseException&gt;(() =&gt; parser.ParseOrThrow(\"b\"));\n</code></pre> <p>The <code>Digit</code> parser handles and returns a single digit character.</p> <pre><code>Assert.AreEqual('3', Digit.ParseOrThrow(\"3\"));\nAssert.Throws&lt;ParseException&gt;(() =&gt; Digit.ParseOrThrow(\"a\"));\n</code></pre> <p>The <code>String</code> parser works with specific strings and fails if the input doesn't match the expected string.</p> <pre><code>Parser&lt;char, string&gt; parser = String(\"foo\");\nAssert.AreEqual(\"foo\", parser.ParseOrThrow(\"foo\"));\nAssert.Throws&lt;ParseException&gt;(() =&gt; parser.ParseOrThrow(\"bar\"));\n</code></pre> <p>The <code>Return</code> (and its synonym <code>FromResult</code>) parser never consumes any input and directly returns the provided value. Conversely, <code>Fail</code> always results in failure without consuming input.</p> <pre><code>Parser&lt;char, int&gt; parser = Return(3);\nAssert.AreEqual(3, parser.ParseOrThrow(\"foo\"));\n\nParser&lt;char, int&gt; parser2 = Fail&lt;int&gt;();\nAssert.Throws&lt;ParseException&gt;(() =&gt; parser2.ParseOrThrow(\"bar\"));\n</code></pre>","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#sequencing-parsers-for-enhanced-parsing","title":"Sequencing Parsers for Enhanced Parsing","text":"<p>One of the strengths of parser combinators is the ability to construct complex parsers from simpler ones. The <code>Then</code> parser facilitates this by creating a new parser that applies two parsers sequentially, discarding the result of the first.</p> <pre><code>Parser&lt;char, string&gt; parser1 = String(\"foo\");\nParser&lt;char, string&gt; parser2 = String(\"bar\");\nParser&lt;char, string&gt; sequencedParser = parser1.Then(parser2);\nAssert.AreEqual(\"bar\", sequencedParser.ParseOrThrow(\"foobar\"));  // \"foo\" is discarded\nAssert.Throws&lt;ParseException&gt;(() =&gt; sequencedParser.ParseOrThrow(\"food\"));\n</code></pre> <p>The <code>Before</code> parser, on the other hand, discards the second result rather than the first.</p> <pre><code>Parser&lt;char, string&gt; parser1 = String(\"foo\");\nParser&lt;char, string&gt; parser2 = String(\"bar\");\nParser&lt;char, string&gt; sequencedParser = parser1.Before(parser2);\nAssert.AreEqual(\"foo\", sequencedParser.ParseOrThrow(\"foobar\"));  // \"bar\" is discarded\nAssert.Throws&lt;ParseException&gt;(() =&gt; sequencedParser.ParseOrThrow(\"food\"));\n</code></pre> <p>The <code>Map</code> parser accomplishes a similar task but preserves both results and applies a transformation function to them. This is particularly useful when you want your parser to yield a custom data structure.</p> <pre><code>Parser&lt;char, string&gt; parser1 = String(\"foo\");\nParser&lt;char, string&gt; parser2 = String(\"bar\");\nParser&lt;char, string&gt; sequencedParser = Map((foo, bar) =&gt; bar + foo, parser1, parser2);\nAssert.AreEqual(\"barfoo\", sequencedParser.ParseOrThrow(\"foobar\"));\nAssert.Throws&lt;ParseException&gt;(() =&gt; sequencedParser.ParseOrThrow(\"food\"));\n</code></pre> <p>The <code>Bind</code> parser uses the outcome of a parser to determine the next parser to execute. This capability is invaluable for parsing context-sensitive languages. For instance, here's a parser that parses any character repeated twice.</p> <pre><code>Parser&lt;char, char&gt; parser = Any.Bind(c =&gt; Char(c));\nAssert.AreEqual('a', parser.ParseOrThrow(\"aa\"));\nAssert.AreEqual('b', parser.ParseOrThrow(\"bb\"));\nAssert.Throws&lt;ParseException&gt;(() =&gt; parser.ParseOrThrow(\"ab\"));\n</code></pre> <p>Modeller.DslParser parsers also support LINQ query syntax, making your parser scripts read like simple imperative code.</p> <pre><code>Parser&lt;char, char&gt; parser =\n    from c in Any\n    from c2 in Char(c)\n    select c2;\n</code></pre> <p>Such parsers are easy to understand, following the logic of \"Run the <code>Any</code> parser and name its result <code>c</code>, then run <code>Char(c)</code> and name its result <code>c2</code>, then return <code>c2</code>.\"</p>","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#navigating-alternative-choices","title":"Navigating Alternative Choices","text":"<p>The <code>Or</code> parser represents a choice between two alternatives. It first attempts the left parser and switches to the right if the left one fails.</p> <pre><code>Parser&lt;char, string&gt; parser = String(\"foo\").Or(String(\"bar\"));\nAssert.AreEqual(\"foo\", parser.ParseOrThrow(\"foo\"));\nAssert.AreEqual(\"bar\", parser.ParseOrThrow(\"bar\"));\nAssert.Throws&lt;ParseException&gt;(() =&gt; parser.ParseOrThrow(\"baz\"));\n</code></pre> <p><code>OneOf</code> is similar to <code>Or</code>, but it accommodates a variable number of arguments. You can replicate the <code>Or</code> parser's behavior using <code>OneOf</code>, as shown below:</p> <pre><code>Parser&lt;char, string&gt; parser = OneOf(String(\"foo\"), String(\"bar\"));\n</code></pre> <p>Keep in mind that if one of the component parsers within <code>Or</code> or <code>OneOf</code> fails after consuming input, the entire parser will fail.</p> <pre><code>Parser&lt;char, string&gt; parser = String(\"food\").Or(String(\"foul\"));\nAssert.Throws&lt;ParseException&gt;(() =&gt; parser.ParseOrThrow(\"foul\"));  // Why didn't it choose the second option?\n</code></pre> <p>This behavior is due to parsers consuming input as they proceed, and Modeller.DslParser does not inherently perform lookahead or backtracking. Backtracking can be enabled using the <code>Try</code> function, as demonstrated below:</p> <pre><code>// Apply Try to the first option, allowing us to revert if it fails\nParser&lt;char, string&gt; parser = Try(String(\"food\")).Or(String(\"foul\"));\nAssert.AreEqual(\"foul\", parser.ParseOrThrow(\"foul\"));\n</code></pre>","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#tackling-recursive-grammars","title":"Tackling Recursive Grammars","text":"<p>Most programming languages, markup languages, or data interchange languages involve some form of recursive structure. However, C# doesn't inherently support recursive values, leading to challenges when referring to a variable that is being initialized. To handle this, Modeller.DslParser introduces the <code>Rec</code> combinator, allowing you to achieve deferred execution of recursive parsers. Here's a simple example where we parse arbitrarily nested parentheses, each containing a single digit.</p> <pre><code>Parser&lt;char, char&gt; expr = null;\nParser&lt;char, char&gt; parenthesized = Char('(')\n    .Then(Rec(() =&gt; expr))  // Utilizing a lambda for (mutual) recursive reference to expr\n    .Before(Char(')'));\nexpr is Digit.Or(parenthesized);\nAssert.AreEqual('1', expr.ParseOrThrow(\"1\"));\nAssert.AreEqual('1', expr.ParseOrThrow(\"(1)\"));\nAssert.AreEqual('1', expr.ParseOrThrow(\"(((1)))\"));\n</code></pre> <p>It's worth noting that Modeller.DslParser does not support left recursion; a parser must consume some input before making a recursive call. Left recursion can lead to stack overflows, as shown in this example:</p> <pre><code>Parser&lt;char, int&gt; arithmetic = null;\nParser&lt;char, int&gt; addExpr = Map(\n    (x, _, y) =&gt; x + y,\n    Rec(() =&gt; arithmetic),\n    Char('+'),\n    Rec(() =&gt; arithmetic)\n);\narithmetic = addExpr.Or(Digit.Select(d =&gt; (int)char.GetNumericValue(d)));\n\narithmetic.Parse(\"2+2\");  // Stack overflow!\n</code></pre>","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#creating-custom-combinators","title":"Creating Custom Combinators","text":"<p>One of the key features of this programming model is the ability to craft your own functions to compose parsers. Modeller.DslParser offers a wide array of higher-level combinators built upon the foundational primitives explained above. For instance, the <code>Between</code> combinator runs a parser enclosed by two others, preserving only the result of the central parser.</p> <pre><code>Parser&lt;TToken, T&gt; InBraces&lt;TToken, T, U, V&gt;(this Parser&lt;TToken, T&gt; parser, Parser&lt;TToken, U&gt; before, Parser&lt;TToken, V&gt; after)\n    =&gt; before.Then(parser).Before(after);\n</code></pre>","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#parsing-expressions-with-operator-precedence","title":"Parsing Expressions with Operator Precedence","text":"<p>Modeller.DslParser equips you with tools for parsing expressions with associative infix operators. The <code>ExpressionParser</code> class builds a parser using a parser for a single expression term and a table of operators with rules for combining expressions.</p>","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#dive-deeper-with-examples","title":"Dive Deeper with Examples","text":"<p>For a deeper understanding of how to utilize Modeller DSL Parser, explore a variety of examples, including parsing subsets of JSON and XML into document structures, in the Modeller.DslParser.Examples project.</p>","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#tips-for-a-smooth-experience","title":"Tips for a Smooth Experience","text":"","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#understanding-variance","title":"Understanding Variance","text":"<p>If you encounter issues like the following code not compiling:</p> <pre><code>class Base {}\nclass Derived : Base {}\n\nParser&lt;char, Base&gt; p = Return(new Derived());  // Cannot implicitly convert type 'Modeller.DslParser.Parser&lt;char, Derived&gt;' to 'Modeller.DslParser.Parser&lt;char, Base&gt;'\n</code></pre> <p>You can resolve this by explicitly setting the type parameter of <code>Select</code> to the supertype:</p> <pre><code>Parser&lt;char, Base&gt; p = Any.Select&lt;Base&gt;(() =&gt; new Derived());\n</code></pre>","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#boosting-performance","title":"Boosting Performance","text":"<p>Modeller.DslParser is designed for speed and minimal memory allocation. To optimize your parser's runtime performance, consider these tips:</p> <ul> <li>Avoid using LINQ query syntax for long queries, as it can generate a significant amount of temporary objects.</li> <li>Minimize backtracking when parsing streaming inputs. The <code>Try</code> function should be used judiciously, as it involves buffering input.</li> <li>Employ specialized parsers, like the <code>Skip*</code> parsers, when the parsing result is not needed. They are typically faster since they avoid value preservation.</li> <li>Build your parser statically when possible, as parser construction can be resource-intensive.</li> <li>Use <code>Map</code> instead of <code>Bind</code> or <code>SelectMany</code> for context-free grammars whenever feasible.</li> </ul>","tags":["codegen"]},{"location":"blog/2023/11/06/parsers/#considering-the-sprache-alternative","title":"Considering the Sprache Alternative","text":"<p>While Modeller.DslParser shares similarities with Sprache, it aims to enhance the parsing experience. Here's why you might prefer Modeller.DslParser:</p> <ul> <li>Modeller.DslParser supports various input token types, not limited to strings, making it suitable for parsing binary protocols and tokenized inputs.</li> <li>It offers better performance and memory efficiency compared to Sprache.</li> <li>Modeller.DslParser provides operator-precedence parsing tools for handling expression grammars with associative infix operators.</li> <li>You can efficiently parse streaming inputs with Modeller.DslParser.</li> </ul> <p>```</p> <p>This revised text provides a comprehensive and detailed explanation of Modeller DSL Parser, its usage, examples, and helpful tips for both beginners and advanced users.</p>","tags":["codegen"]},{"location":"blog/2023/11/01/project-directory-structures/","title":"Project Directory Structures","text":"<p>Various methods exist for organizing a project's directory structure, but I have a particular preference. Prior to delving into that, let's examine the conventional approaches often suggested by templates and demoware and why they may not be the most advantageous choice.</p>","tags":["design"]},{"location":"blog/2023/11/01/project-directory-structures/#organization-by-technical-concern","title":"Organization by Technical Concern","text":"<p>To illustrate this point, consider the analogy of a library\u2014a repository of books available for reading, borrowing, and taking home. In a library, books are methodically arranged on shelves according to their respective categories. This systematic arrangement simplifies the process of finding a specific book and, at the same time, enables the discovery of related literature.</p> <p>Similarly, in the realm of software development, projects are frequently organized based on technical considerations. This methodology entails the creation of distinct categories such as <code>Repositories</code>, <code>Services</code>, <code>Validators</code>, and <code>Mappers</code> to classify and manage various project components.</p> <p>However, this approach can present challenges in terms of comprehending and navigating the codebase, particularly for those who are unfamiliar with it. The task of locating code associated with specific features becomes notably convoluted, leading to an increased cognitive load when implementing updates, conducting refactoring, or simply navigating the codebase.</p> <p>It is worth noting that when components are organized in this fashion, they often encompass a wide range of functionalities. For instance, a Repository or a Service may contain code relevant to multiple features.</p>","tags":["design"]},{"location":"blog/2023/11/01/project-directory-structures/#the-mvc-conundrum","title":"The MVC Conundrum","text":"<p>Certain template structures inherently foster the segregation of technological components, further exacerbating the aforementioned challenges. A prime exemplar of this is the Model-View-Controller (MVC) architectural pattern.</p> <p>When embarking on the development of a new MVC-based API, it is customary to create dedicated folders for Controllers, Models, and Views. This inherent design encourages the segregation of components based on their technological function once more, perpetuating the navigational and cognitive hurdles.</p>","tags":["design"]},{"location":"blog/2023/11/01/project-directory-structures/#my-preferred-approach","title":"My Preferred Approach","text":"<p>While the library's category system simplifies finding and accessing books, it may not be the optimal model for source code organization, in my view.</p> <p>In the context of source code, a single feature often spans multiple files, each with distinct responsibilities or categories. When we access code for maintenance, we generally do so within the context of a feature, or more precisely, within a narrow vertical slice. So, how should we organize our code instead? The rule is straightforward.</p> <p>Organize by action/utility.</p> <p>What does this entail?</p> <ul> <li>Deliberate consideration of how the code will be used, accessed, and maintained.</li> <li>Thoughtful examination of which code components will naturally evolve in tandem.</li> <li>The conscious effort to reduce the cognitive load associated with future actions.</li> </ul> <p>In essence, this encapsulates the concept of Feature Folders. Additionally, this approach can be further refined through the combined utilization of Area and Feature folders.</p>","tags":["design"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/","title":"Advanced Security Addendum: Scopes, Claims, and Token Behavior in Blazor Server","text":"<p>This document serves as a security addendum to the main article \"Scopes, Claims, and Token Behavior in Blazor Server with Entra ID and Microsoft Graph\". It covers advanced security topics and implementation patterns that extend beyond the foundational concepts discussed in the original document.</p> <p>WARNING: This addendum was written by AI and has yet to be fully vetted.</p> <p>Prerequisites: This addendum assumes familiarity with the concepts, configurations, and patterns covered in the main document, particularly the sections on token types, MSAL configuration, and WSO2 API Manager integration.</p>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#continuous-access-evaluation-cae-implementation","title":"Continuous Access Evaluation (CAE) Implementation","text":"<p>Building upon the token validation concepts discussed in the original document, Continuous Access Evaluation (CAE) provides real-time security policy enforcement that goes beyond traditional token expiration.</p>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#understanding-cae-in-the-blazor-server-context","title":"Understanding CAE in the Blazor Server Context","text":"<p>As covered in the main document's token lifecycle section, traditional access tokens have fixed expiration times. CAE enhances this by enabling immediate token revocation based on critical security events:</p> <ul> <li>User account disabled/deleted</li> <li>Password changes</li> <li>High-risk sign-in detection</li> <li>Location-based policy violations</li> </ul>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#cae-configuration-for-blazor-server","title":"CAE Configuration for Blazor Server","text":"<p>Extending the MSAL configuration shown in the original document:</p> <pre><code>// Building upon the original AddMicrosoftIdentityWebApp configuration\nbuilder.Services.AddAuthentication(OpenIdConnectDefaults.AuthenticationScheme)\n    .AddMicrosoftIdentityWebApp(options =&gt;\n    {\n        builder.Configuration.GetSection(\"AzureAd\").Bind(options);\n        options.UsePkce = true;\n        options.SaveTokens = false;\n\n        // CAE-specific configuration\n        options.Events.OnTokenValidated = async context =&gt;\n        {\n            // Enable CAE claims processing\n            var identity = (ClaimsIdentity)context.Principal!.Identity!;\n            if (identity.HasClaim(\"xms_cc\", \"cp1\"))\n            {\n                // Token supports CAE - log for monitoring\n                var logger = context.HttpContext.RequestServices\n                    .GetRequiredService&lt;ILogger&lt;Program&gt;&gt;();\n                logger.LogInformation(\"CAE-enabled token received for user {UserId}\", \n                    identity.FindFirst(\"sub\")?.Value);\n            }\n        };\n    })\n    .EnableTokenAcquisitionToCallDownstreamApi(options =&gt;\n    {\n        // Request CAE-capable tokens\n        options.LongRunningWebApi = true; // Enables CAE support\n    }, new[] { \"User.Read\", \"Mail.Read\" })\n    .AddMicrosoftGraph(builder.Configuration.GetSection(\"Graph\"))\n    .AddDistributedTokenCaches();\n</code></pre>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#cae-aware-component-implementation","title":"CAE-Aware Component Implementation","text":"<p>Extending the GraphComponent pattern from the original document:</p> <pre><code>[AuthorizeForScopes(Scopes = new[] { \"User.Read\" })]\npublic class CAEAwareGraphComponent : ComponentBase\n{\n    [Inject] GraphServiceClient GraphClient { get; set; } = default!;\n    [Inject] ILogger&lt;CAEAwareGraphComponent&gt; Logger { get; set; } = default!;\n\n    private string? displayName;\n    private bool isCaeEnabled;\n\n    protected override async Task OnInitializedAsync()\n    {\n        try\n        {\n            // Check if current token supports CAE\n            var authState = await AuthenticationStateProvider.GetAuthenticationStateAsync();\n            isCaeEnabled = authState.User.HasClaim(\"xms_cc\", \"cp1\");\n\n            var me = await GraphClient.Me.Request().GetAsync();\n            displayName = me.DisplayName;\n\n            Logger.LogInformation(\"Profile loaded with CAE support: {CaeEnabled}\", isCaeEnabled);\n        }\n        catch (ServiceException ex) when (ex.Error?.Code == \"InvalidAuthenticationToken\")\n        {\n            // CAE may have revoked the token - handle gracefully\n            Logger.LogWarning(\"Token revoked via CAE for user session\");\n            await HandleTokenRevocation();\n        }\n    }\n\n    private async Task HandleTokenRevocation()\n    {\n        // Implement CAE token revocation handling\n        displayName = \"Session expired - please sign in again\";\n        // Optionally trigger re-authentication\n    }\n}\n</code></pre>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#cae-integration-with-wso2-api-manager","title":"CAE Integration with WSO2 API Manager","text":"<p>Referencing the WSO2 integration patterns from the original document, CAE requires additional gateway configuration:</p> <pre><code># WSO2 APIM CAE Configuration Extension\n[apim.oauth_config]\nenable_cae_validation = true\ncae_endpoint = \"https://login.microsoftonline.com/{tenant}/oauth2/v2.0/cae\"\n\n[apim.jwt_config]\n# Validate CAE claims in JWT tokens\nrequired_claims = [\"xms_cc\", \"xms_tcdt\"]\n</code></pre>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#proof-of-possession-pop-token-implementation","title":"Proof of Possession (PoP) Token Implementation","text":"<p>While the original document covers standard bearer tokens, Proof of Possession (PoP) tokens provide enhanced security by cryptographically binding tokens to specific clients.</p>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#when-to-implement-pop-tokens","title":"When to Implement PoP Tokens","text":"<p>PoP tokens should be considered for: - High-value API operations (financial transactions, sensitive data access) - Zero Trust architecture implementations - Compliance requirements (PCI DSS, FIDO2)</p>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#pop-token-configuration","title":"PoP Token Configuration","text":"<p>Extending the token acquisition patterns from the main document:</p> <pre><code>// Enhanced token acquisition with PoP support\npublic class PopTokenService\n{\n    private readonly ITokenAcquisition _tokenAcquisition;\n    private readonly ILogger&lt;PopTokenService&gt; _logger;\n\n    public PopTokenService(ITokenAcquisition tokenAcquisition, ILogger&lt;PopTokenService&gt; logger)\n    {\n        _tokenAcquisition = tokenAcquisition;\n        _logger = logger;\n    }\n\n    public async Task&lt;string&gt; AcquirePopTokenAsync(string[] scopes, string httpMethod, string url)\n    {\n        try\n        {\n            // Generate key pair for PoP\n            using var rsa = RSA.Create(2048);\n            var publicKey = Convert.ToBase64String(rsa.ExportRSAPublicKey());\n\n            // Create PoP token request\n            var popTokenRequest = new AuthenticationRequestParameters\n            {\n                Scopes = scopes,\n                ExtraQueryParameters = new Dictionary&lt;string, string&gt;\n                {\n                    [\"token_type\"] = \"pop\",\n                    [\"req_cnf\"] = CreateConfirmationClaim(publicKey, httpMethod, url)\n                }\n            };\n\n            var result = await _tokenAcquisition.GetAccessTokenForUserAsync(\n                scopes, \n                authenticationScheme: OpenIdConnectDefaults.AuthenticationScheme);\n\n            _logger.LogInformation(\"PoP token acquired for scopes: {Scopes}\", string.Join(\", \", scopes));\n            return result;\n        }\n        catch (Exception ex)\n        {\n            _logger.LogError(ex, \"Failed to acquire PoP token\");\n            throw;\n        }\n    }\n\n    private string CreateConfirmationClaim(string publicKey, string httpMethod, string url)\n    {\n        var confirmation = new\n        {\n            jwk = new\n            {\n                kty = \"RSA\",\n                n = publicKey,\n                e = \"AQAB\"\n            },\n            htm = httpMethod,\n            htu = url,\n            jti = Guid.NewGuid().ToString()\n        };\n\n        return Convert.ToBase64String(Encoding.UTF8.GetBytes(JsonSerializer.Serialize(confirmation)));\n    }\n}\n</code></pre>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#pop-token-usage-in-components","title":"PoP Token Usage in Components","text":"<pre><code>[AuthorizeForScopes(Scopes = new[] { \"https://graph.microsoft.com/Mail.Read\" })]\npublic class PopEnabledMailComponent : ComponentBase\n{\n    [Inject] PopTokenService PopTokenService { get; set; } = default!;\n    [Inject] HttpClient HttpClient { get; set; } = default!;\n\n    private async Task&lt;string&gt; CallGraphWithPopToken()\n    {\n        var scopes = new[] { \"https://graph.microsoft.com/Mail.Read\" };\n        var url = \"https://graph.microsoft.com/v1.0/me/messages\";\n\n        // Acquire PoP token bound to this specific request\n        var popToken = await PopTokenService.AcquirePopTokenAsync(scopes, \"GET\", url);\n\n        // Use PoP token in Authorization header\n        HttpClient.DefaultRequestHeaders.Authorization = \n            new System.Net.Http.Headers.AuthenticationHeaderValue(\"PoP\", popToken);\n\n        var response = await HttpClient.GetAsync(url);\n        return await response.Content.ReadAsStringAsync();\n    }\n}\n</code></pre>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#cookie-security-and-aspnet-core-data-protection","title":"Cookie Security and ASP.NET Core Data Protection","text":"<p>Complementing the authentication configuration covered in the original document, proper cookie security and data protection are critical for Blazor Server applications.</p>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#understanding-aspnet-core-data-protection","title":"Understanding ASP.NET Core Data Protection","text":"<p>What is Data Protection? The ASP.NET Core Data Protection API provides a simple, easy-to-use cryptographic API that applications can use to protect data, including temporary data that should not be persisted forever. It's designed to serve as the replacement for the <code>&lt;machineKey&gt;</code> element in ASP.NET 1.x - 4.x.</p> <p>Why is Data Protection Critical for Blazor Server + Entra ID?</p> <p>In the context of Blazor Server applications with Entra ID authentication, Data Protection is essential for several security-critical scenarios:</p> <ol> <li> <p>Authentication Cookie Protection: As discussed in the original document's authentication flow, Blazor Server maintains user sessions through authentication cookies. These cookies contain sensitive authentication state that must be encrypted and tamper-proof.</p> </li> <li> <p>Anti-Forgery Token Security: Blazor Server uses anti-forgery tokens to prevent CSRF attacks on SignalR connections. These tokens must be cryptographically secure and validated.</p> </li> <li> <p>Sensitive Claims Storage: When using <code>IClaimsTransformation</code> (covered in the original document), transformed claims may need temporary encrypted storage to prevent tampering.</p> </li> <li> <p>Circuit State Protection: Blazor Server maintains user state in server-side circuits. Sensitive data within these circuits requires protection when serialized or cached.</p> </li> <li> <p>Token Cache Encryption: While the original document shows <code>.AddInMemoryTokenCaches()</code>, production environments often require encrypted token storage across multiple server instances.</p> </li> </ol> <p>Security Risks Without Proper Data Protection: - Session Hijacking: Unprotected authentication cookies can be stolen and replayed - Claims Tampering: Modified claims could lead to privilege escalation - Cross-Site Request Forgery: Weak anti-forgery tokens compromise SignalR security - Data Leakage: Unencrypted sensitive data in logs, memory dumps, or storage</p>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#data-protection-architecture-in-blazor-server","title":"Data Protection Architecture in Blazor Server","text":"<p>Key Components: - Key Ring: Manages encryption keys with automatic rotation - Data Protectors: Provide purpose-specific encryption/decryption - Key Escrow: Enables key recovery for compliance scenarios - Persistence Layer: Stores keys securely across application restarts</p> <p>Integration with Entra ID Authentication Flow: <pre><code>graph TD\n    A[User Authentication] --&gt; B[Entra ID Token Response]\n    B --&gt; C[Data Protection: Encrypt Auth Cookie]\n    C --&gt; D[Store in Browser Cookie]\n    D --&gt; E[Subsequent Requests]\n    E --&gt; F[Data Protection: Decrypt Auth Cookie]\n    F --&gt; G[Validate Claims &amp; Session]\n    G --&gt; H[Access Blazor Components]</code></pre></p>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#enhanced-cookie-security-configuration","title":"Enhanced Cookie Security Configuration","text":"<p>Building upon the basic authentication setup from the main document:</p> <pre><code>// Enhanced cookie security configuration\nbuilder.Services.ConfigureApplicationCookie(options =&gt;\n{\n    // Security headers\n    options.Cookie.SecurePolicy = CookieSecurePolicy.Always;\n    options.Cookie.SameSite = SameSiteMode.Strict;\n    options.Cookie.HttpOnly = true;\n    options.Cookie.IsEssential = true;\n\n    // Timing configuration\n    options.ExpireTimeSpan = TimeSpan.FromHours(1);\n    options.SlidingExpiration = true;\n    options.AccessDeniedPath = \"/Account/AccessDenied\";\n    options.LoginPath = \"/Account/SignIn\";\n\n    // Enhanced security events\n    options.Events.OnValidatePrincipal = async context =&gt;\n    {\n        // Validate security stamp for immediate logout on password change\n        var securityStampValidator = context.HttpContext.RequestServices\n            .GetRequiredService&lt;ISecurityStampValidator&gt;();\n        await securityStampValidator.ValidateAsync(context);\n    };\n});\n</code></pre>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#data-protection-api-configuration","title":"Data Protection API Configuration","text":"<p>Development Environment Configuration: <pre><code>// Development - keys stored locally\nif (builder.Environment.IsDevelopment())\n{\n    builder.Services.AddDataProtection()\n        .PersistKeysToFileSystem(new DirectoryInfo(@\".\\keys\"))\n        .SetApplicationName(\"BlazorServerApp-Dev\");\n}\n</code></pre></p> <p>Production Environment Configuration: <pre><code>// Production - enterprise-grade key management\nif (builder.Environment.IsProduction())\n{\n    builder.Services.AddDataProtection()\n        // Persist keys to Azure Blob Storage for multi-instance deployments\n        .PersistKeysToAzureBlobStorage(connectionString, containerName, blobName)\n\n        // Protect keys with Azure Key Vault HSM\n        .ProtectKeysWithAzureKeyVault(keyVaultUri, keyName)\n\n        // Application isolation - critical for multi-tenant scenarios\n        .SetApplicationName(\"BlazorServerApp-Prod\")\n\n        // Key rotation policy - balance security vs performance\n        .SetDefaultKeyLifetime(TimeSpan.FromDays(90))\n\n        // Compliance requirement - key escrow for data recovery\n        .AddKeyEscrowSink&lt;CustomKeyEscrowSink&gt;()\n\n        // Additional security - require authentication for key access\n        .AddKeyManagementOptions(options =&gt;\n        {\n            options.AuthenticatedEncryptorConfiguration = new AuthenticatedEncryptorConfiguration\n            {\n                EncryptionAlgorithm = EncryptionAlgorithm.AES_256_CBC,\n                ValidationAlgorithm = ValidationAlgorithm.HMACSHA256\n            };\n        });\n}\n\n// Custom data protection for sensitive claims\nbuilder.Services.AddSingleton&lt;IClaimsProtectionService, ClaimsProtectionService&gt;();\n</code></pre></p> <p>Why These Configuration Choices Matter:</p> <ol> <li> <p>Azure Blob Storage Persistence: Ensures keys are available across multiple server instances, preventing authentication failures during deployments or scaling events.</p> </li> <li> <p>Azure Key Vault Protection: Hardware Security Module (HSM) protection for master keys, meeting compliance requirements for financial and healthcare applications.</p> </li> <li> <p>Application Name Isolation: Prevents key sharing between different applications or environments, critical for multi-tenant scenarios.</p> </li> <li> <p>90-Day Key Lifetime: Balances security (regular key rotation) with performance (reduces key generation overhead).</p> </li> <li> <p>Key Escrow: Enables data recovery for compliance scenarios while maintaining security.</p> </li> </ol>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#secure-claims-protection-service","title":"Secure Claims Protection Service","text":"<p>Purpose: Extends the <code>IClaimsTransformation</code> pattern from the original document by providing encrypted storage for sensitive claims that need to persist beyond the immediate authentication context.</p> <p>Use Cases: - Caching expensive claims transformations (e.g., role lookups from external systems) - Storing sensitive user attributes that shouldn't appear in plain text logs - Protecting claims during circuit serialization in Blazor Server</p> <pre><code>public interface IClaimsProtectionService\n{\n    string ProtectClaims(ClaimsPrincipal principal);\n    ClaimsPrincipal UnprotectClaims(string protectedClaims);\n    Task&lt;string&gt; ProtectClaimsWithExpirationAsync(ClaimsPrincipal principal, TimeSpan expiration);\n    Task&lt;ClaimsPrincipal?&gt; UnprotectClaimsWithExpirationAsync(string protectedClaims);\n}\n\npublic class ClaimsProtectionService : IClaimsProtectionService\n{\n    private readonly IDataProtector _protector;\n    private readonly IDataProtector _timedProtector;\n    private readonly ILogger&lt;ClaimsProtectionService&gt; _logger;\n\n    public ClaimsProtectionService(IDataProtectionProvider provider, ILogger&lt;ClaimsProtectionService&gt; logger)\n    {\n        // Purpose-specific protectors for different claim scenarios\n        _protector = provider.CreateProtector(\"ClaimsProtection.v1\");\n        _timedProtector = provider.CreateProtector(\"ClaimsProtection.Timed.v1\").ToTimeLimitedDataProtector();\n        _logger = logger;\n    }\n\n    public string ProtectClaims(ClaimsPrincipal principal)\n    {\n        try\n        {\n            // Filter sensitive claims - don't protect system claims\n            var protectableClaims = principal.Claims\n                .Where(c =&gt; !IsSystemClaim(c.Type))\n                .Select(c =&gt; new ClaimData { Type = c.Type, Value = c.Value })\n                .ToArray();\n\n            var claimsPackage = new ProtectedClaimsPackage\n            {\n                Claims = protectableClaims,\n                UserId = principal.FindFirst(\"sub\")?.Value,\n                IssuedAt = DateTimeOffset.UtcNow,\n                AuthenticationType = principal.Identity?.AuthenticationType\n            };\n\n            var json = JsonSerializer.Serialize(claimsPackage);\n            var protectedData = _protector.Protect(json);\n\n            _logger.LogDebug(\"Protected {ClaimCount} claims for user {UserId}\",\n                protectableClaims.Length, claimsPackage.UserId);\n\n            return protectedData;\n        }\n        catch (Exception ex)\n        {\n            _logger.LogError(ex, \"Failed to protect claims\");\n            throw;\n        }\n    }\n\n    public ClaimsPrincipal UnprotectClaims(string protectedClaims)\n    {\n        try\n        {\n            var json = _protector.Unprotect(protectedClaims);\n            var claimsPackage = JsonSerializer.Deserialize&lt;ProtectedClaimsPackage&gt;(json);\n\n            if (claimsPackage?.Claims == null)\n            {\n                throw new InvalidOperationException(\"Invalid claims package\");\n            }\n\n            var claims = claimsPackage.Claims.Select(c =&gt; new Claim(c.Type, c.Value));\n            var identity = new ClaimsIdentity(claims, claimsPackage.AuthenticationType ?? \"DataProtection\");\n\n            _logger.LogDebug(\"Unprotected {ClaimCount} claims for user {UserId}\",\n                claimsPackage.Claims.Length, claimsPackage.UserId);\n\n            return new ClaimsPrincipal(identity);\n        }\n        catch (Exception ex)\n        {\n            _logger.LogError(ex, \"Failed to unprotect claims\");\n            throw;\n        }\n    }\n\n    public async Task&lt;string&gt; ProtectClaimsWithExpirationAsync(ClaimsPrincipal principal, TimeSpan expiration)\n    {\n        // Implementation for time-limited claim protection\n        var claimsData = ProtectClaims(principal);\n        return _timedProtector.Protect(claimsData, expiration);\n    }\n\n    public async Task&lt;ClaimsPrincipal?&gt; UnprotectClaimsWithExpirationAsync(string protectedClaims)\n    {\n        try\n        {\n            var claimsData = _timedProtector.Unprotect(protectedClaims);\n            return UnprotectClaims(claimsData);\n        }\n        catch (CryptographicException)\n        {\n            // Time-limited protection expired\n            _logger.LogInformation(\"Time-limited claims protection expired\");\n            return null;\n        }\n    }\n\n    private bool IsSystemClaim(string claimType)\n    {\n        // Don't protect system/infrastructure claims\n        var systemClaims = new[] { \"iss\", \"aud\", \"exp\", \"iat\", \"nbf\", \"sub\", \"jti\" };\n        return systemClaims.Contains(claimType);\n    }\n}\n\n// Supporting data structures\npublic class ProtectedClaimsPackage\n{\n    public ClaimData[] Claims { get; set; } = Array.Empty&lt;ClaimData&gt;();\n    public string? UserId { get; set; }\n    public DateTimeOffset IssuedAt { get; set; }\n    public string? AuthenticationType { get; set; }\n}\n\npublic class ClaimData\n{\n    public string Type { get; set; } = string.Empty;\n    public string Value { get; set; } = string.Empty;\n}\n</code></pre> <p>Integration with Claims Transformation: <pre><code>// Enhanced claims transformer using data protection\npublic class SecureClaimsTransformer : IClaimsTransformation\n{\n    private readonly IClaimsProtectionService _claimsProtection;\n    private readonly IMemoryCache _cache;\n\n    public async Task&lt;ClaimsPrincipal&gt; TransformAsync(ClaimsPrincipal principal)\n    {\n        var userId = principal.FindFirst(\"sub\")?.Value;\n        if (userId == null) return principal;\n\n        // Check for cached protected claims\n        var cacheKey = $\"protected_claims_{userId}\";\n        if (_cache.TryGetValue(cacheKey, out string? cachedProtectedClaims))\n        {\n            var cachedPrincipal = await _claimsProtection.UnprotectClaimsWithExpirationAsync(cachedProtectedClaims);\n            if (cachedPrincipal != null)\n            {\n                return MergeClaims(principal, cachedPrincipal);\n            }\n        }\n\n        // Perform expensive claims transformation\n        var transformedPrincipal = await PerformExpensiveClaimsLookup(principal);\n\n        // Cache the protected claims\n        var protectedClaims = await _claimsProtection.ProtectClaimsWithExpirationAsync(\n            transformedPrincipal, TimeSpan.FromMinutes(15));\n        _cache.Set(cacheKey, protectedClaims, TimeSpan.FromMinutes(15));\n\n        return transformedPrincipal;\n    }\n}\n</code></pre></p>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#integration-with-original-document-patterns","title":"Integration with Original Document Patterns","text":"","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#enhanced-security-headers-middleware","title":"Enhanced Security Headers Middleware","text":"<p>Extending the security considerations from the main document:</p> <pre><code>// Comprehensive security headers for Blazor Server\napp.Use(async (context, next) =&gt;\n{\n    var headers = context.Response.Headers;\n\n    // Basic security headers\n    headers.Add(\"X-Frame-Options\", \"DENY\");\n    headers.Add(\"X-Content-Type-Options\", \"nosniff\");\n    headers.Add(\"Referrer-Policy\", \"strict-origin-when-cross-origin\");\n\n    // Enhanced CSP for Blazor Server with SignalR\n    headers.Add(\"Content-Security-Policy\", \n        \"default-src 'self'; \" +\n        \"script-src 'self' 'unsafe-inline' 'unsafe-eval'; \" +\n        \"style-src 'self' 'unsafe-inline'; \" +\n        \"connect-src 'self' wss: https:; \" +\n        \"img-src 'self' data: https:;\");\n\n    // HSTS for production\n    if (app.Environment.IsProduction())\n    {\n        headers.Add(\"Strict-Transport-Security\", \"max-age=31536000; includeSubDomains\");\n    }\n\n    await next();\n});\n</code></pre>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#wso2-integration-with-advanced-security","title":"WSO2 Integration with Advanced Security","text":"<p>Extending the WSO2 patterns from the original document:</p> <pre><code># Enhanced WSO2 configuration for advanced security features\n[apim.oauth_config]\nenable_pop_validation = true\nenable_cae_validation = true\ntoken_binding_validation = true\n\n[apim.security]\nenable_mtls = true\nclient_certificate_validation = true\n</code></pre>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#monitoring-and-compliance","title":"Monitoring and Compliance","text":"","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#security-event-logging","title":"Security Event Logging","text":"<pre><code>// Enhanced logging for security events\npublic class SecurityEventLogger\n{\n    private readonly ILogger&lt;SecurityEventLogger&gt; _logger;\n\n    public void LogTokenAcquisition(string userId, string[] scopes, bool caeEnabled, bool popEnabled)\n    {\n        _logger.LogInformation(\"Token acquired - User: {UserId}, Scopes: {Scopes}, CAE: {CaeEnabled}, PoP: {PopEnabled}\",\n            userId, string.Join(\",\", scopes), caeEnabled, popEnabled);\n    }\n\n    public void LogSecurityViolation(string userId, string violation, string details)\n    {\n        _logger.LogWarning(\"Security violation - User: {UserId}, Violation: {Violation}, Details: {Details}\",\n            userId, violation, details);\n    }\n}\n</code></pre>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#summary","title":"Summary","text":"<p>This addendum extends the foundational security concepts from the main document with advanced implementation patterns for:</p> <ul> <li>Continuous Access Evaluation for real-time policy enforcement</li> <li>Proof of Possession tokens for enhanced API security</li> <li>Cookie security and Data Protection for comprehensive application security</li> </ul> <p>These patterns should be implemented based on your specific security requirements and compliance needs, building upon the solid foundation established in the original document's authentication and authorization patterns.</p>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#see-also","title":"See Also","text":"","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#related-documentation","title":"Related Documentation","text":"<ul> <li>[Main Article]: Scopes, Claims, and Token Behavior in Blazor Server with Entra ID and Microsoft Graph - Foundational concepts and basic implementation patterns</li> <li>Microsoft Learn: Threat mitigation guidance for ASP.NET Core Blazor interactive server-side rendering</li> <li>Microsoft Learn: Configure JWT bearer authentication in ASP.NET Core</li> </ul>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#advanced-security-resources","title":"Advanced Security Resources","text":"<ul> <li>OWASP: .NET Security Cheat Sheet</li> <li>Microsoft Security: Zero Trust security model</li> <li>NIST: Cybersecurity Framework</li> </ul>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#citations-and-references","title":"Citations and References","text":"","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#microsoft-official-documentation","title":"Microsoft Official Documentation","text":"<ol> <li>Microsoft Learn - \"Continuous Access Evaluation in Microsoft Entra ID\" - https://learn.microsoft.com/en-us/entra/identity/conditional-access/concept-continuous-access-evaluation</li> <li>Microsoft Learn - \"Microsoft identity platform and OAuth 2.0 authorization code flow\" - https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-auth-code-flow</li> <li>Microsoft Learn - \"Acquire and cache tokens with Microsoft Authentication Library (MSAL)\" - https://learn.microsoft.com/en-us/entra/identity-platform/msal-acquire-cache-tokens</li> <li>Microsoft Learn - \"Data Protection in ASP.NET Core\" - https://learn.microsoft.com/en-us/aspnet/core/security/data-protection/</li> </ol>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#security-standards-and-specifications","title":"Security Standards and Specifications","text":"<ol> <li>RFC 7800 - \"Proof-of-Possession Key Semantics for JSON Web Tokens (JWTs)\" - https://tools.ietf.org/html/rfc7800</li> <li>RFC 8705 - \"OAuth 2.0 Mutual-TLS Client Authentication and Certificate-Bound Access Tokens\" - https://tools.ietf.org/html/rfc8705</li> <li>OWASP Top 10 - \"Web Application Security Risks\" - https://owasp.org/www-project-top-ten/</li> </ol>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#implementation-guidance-sources","title":"Implementation Guidance Sources","text":"<ol> <li>Microsoft Security Blog - \"Continuous Access Evaluation: A new security paradigm\" - Referenced for CAE implementation patterns</li> <li>ASP.NET Core Security Documentation - Cookie security and authentication configuration patterns</li> <li>WSO2 Documentation - \"API Manager Security\" - Integration patterns for enterprise API gateways</li> </ol>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#research-and-analysis-sources","title":"Research and Analysis Sources","text":"<ol> <li>GitHub Issues - dotnet/aspnetcore repository - Blazor Server security discussions and threat mitigation guidance</li> <li>Microsoft Security Response Center - Security advisories and vulnerability disclosures for .NET applications</li> <li>NIST Special Publication 800-63B - \"Authentication and Lifecycle Management\" - Multi-factor authentication and token binding guidance</li> </ol>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/advanced-security-addendum-scopes-claims-and-token-behavior-in-blazor-server/#disclaimer","title":"Disclaimer","text":"<p>Important Note: The code examples and configuration patterns in this addendum are based on current Microsoft documentation, security best practices, and established standards as of August 2025. However, some advanced implementation details (particularly around PoP token generation and CAE event handling) represent recommended patterns that should be validated against the latest Microsoft documentation and tested thoroughly in your specific environment.</p> <p>Security Recommendation: Always consult the latest official Microsoft documentation and conduct security testing before implementing these patterns in production environments. Security requirements and implementation details may vary based on your specific compliance, regulatory, and organizational security policies.</p> <p>\ud83d\udccc Next Steps: Review your current implementation against these advanced patterns and implement them incrementally based on your security posture requirements and compliance obligations.</p>","tags":["design","advanced-security"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/","title":"Scopes, Claims, and Token Behavior in Blazor Server with Entra ID and Microsoft Graph","text":"<p>This document provides a detailed explanation of scopes, claims, and tokens in a Blazor Server application integrated with Microsoft Entra ID (Azure AD) and Microsoft Graph. It also covers their behavioral effects when APIs are accessed via a gateway, and explains the role of the issuer and authority. This guidance is intended for technical IT staff including project managers, architects, security teams, and lead developers.</p>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#scopes-vs-claims","title":"Scopes vs Claims","text":"","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#scopes-permissions","title":"Scopes (Permissions)","text":"<ul> <li>Definition: Scopes represent what an application is allowed to do.</li> <li>Where used: Requested during token acquisition and validated by the API or gateway.</li> <li>Behavioral effect:</li> <li>They appear in the access token as the <code>scp</code> claim.</li> <li>APIs enforce scopes to decide if a request is authorized.</li> <li>Example:   <pre><code>\"scp\": \"User.Read Mail.Read\"\n</code></pre>   This means the token holder can read profile info and email.</li> </ul>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#claims-identity-attributes","title":"Claims (Identity Attributes)","text":"<ul> <li>Definition: Claims describe who the user is and additional attributes.</li> <li>Where used: Issued in ID tokens and access tokens.</li> <li>Behavioral effect:</li> <li>Claims affect application logic (e.g., roles, display name, tenant ID).</li> <li>They do not change what APIs you can call (that is scope-driven).</li> </ul> <p>Example claims in an ID token: <pre><code>{\n  \"name\": \"Alice Example\",\n  \"preferred_username\": \"alice@contoso.com\",\n  \"roles\": [\"Admin\", \"User\"],\n  \"iss\": \"https://login.microsoftonline.com/{tenantId}/v2.0\"\n}\n</code></pre></p>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#token-types-and-their-purpose","title":"Token Types and Their Purpose","text":"","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#id-token","title":"ID Token","text":"<ul> <li>Confirms who the user is.</li> <li>Contains claims used for personalization and authorization inside Blazor.</li> <li>Not used when calling APIs.</li> </ul>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#access-token","title":"Access Token","text":"<ul> <li>Confirms what the client app is allowed to do.</li> <li>Contains <code>scp</code> claim (scopes) and some identity claims.</li> <li>Sent in the <code>Authorization: Bearer &lt;token&gt;</code> header when calling Microsoft Graph or your APIs.</li> </ul>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#refresh-token","title":"Refresh Token","text":"<ul> <li>Used to silently request new access tokens when old ones expire.</li> <li>Managed automatically by MSAL in Blazor with <code>.AddInMemoryTokenCaches()</code>.</li> <li>Not sent to your APIs.</li> </ul>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#how-iclaimstransformation-fits-in","title":"How <code>IClaimsTransformation</code> Fits In","text":"<ul> <li>Runs after authentication to transform the in-app <code>ClaimsPrincipal</code>.</li> <li>Does not modify the actual ID token or access token issued by Entra.</li> <li>Useful for mapping Entra claims (e.g., <code>groups</code>) into app-specific roles.</li> </ul> <p>Example: <pre><code>public class CustomClaimsTransformer : IClaimsTransformation\n{\n    public Task&lt;ClaimsPrincipal&gt; TransformAsync(ClaimsPrincipal principal)\n    {\n        var identity = (ClaimsIdentity)principal.Identity!;\n        if (identity.HasClaim(c =&gt; c.Type == \"groups\" &amp;&amp; c.Value == \"group-id-here\"))\n        {\n            identity.AddClaim(new Claim(ClaimTypes.Role, \"FinanceAdmin\"));\n        }\n        return Task.FromResult(principal);\n    }\n}\n</code></pre></p> <p>\ud83d\udc49 This affects only Blazor\u2019s local authorization checks, not downstream API calls.</p>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#authority-and-issuer","title":"Authority and Issuer","text":"<ul> <li>Authority: URL MSAL uses to authenticate and request tokens.   <pre><code>\"Authority\": \"https://login.microsoftonline.com/{tenantId}/v2.0\"\n</code></pre></li> <li>Provides metadata endpoints (keys, discovery, etc.).</li> <li> <p>Defines the tenant/policy for authentication.</p> </li> <li> <p>Issuer (<code>iss</code>) claim: Inside tokens, confirms who issued the token.   <pre><code>\"iss\": \"https://login.microsoftonline.com/contosoTenantId/v2.0\"\n</code></pre></p> </li> <li>APIs and gateways validate that the <code>iss</code> matches the configured authority.</li> <li>Prevents tokens from other tenants/issuers being accepted.</li> </ul>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#example-configuration-and-flow-in-blazor","title":"Example Configuration and Flow in Blazor","text":"<p>Startup configuration: <pre><code>builder.Services.AddAuthentication(OpenIdConnectDefaults.AuthenticationScheme)\n    .AddMicrosoftIdentityWebApp(builder.Configuration.GetSection(\"AzureAd\"))\n    .EnableTokenAcquisitionToCallDownstreamApi(new[] { \"User.Read\", \"Mail.Read\" })\n    .AddMicrosoftGraph(builder.Configuration.GetSection(\"Graph\"))\n    .AddInMemoryTokenCaches();\n</code></pre></p> <p>Component requiring Graph permissions: <pre><code>[AuthorizeForScopes(Scopes = new[] { \"User.Read\" })]\npublic class GraphComponent : ComponentBase\n{\n    [Inject]\n    GraphServiceClient GraphClient { get; set; } = default!;\n\n    private string? displayName;\n\n    protected override async Task OnInitializedAsync()\n    {\n        var me = await GraphClient.Me.Request().GetAsync();\n        displayName = me.DisplayName;\n    }\n}\n</code></pre></p> <p>Behavior: - <code>[AuthorizeForScopes]</code> ensures the access token in cache has <code>User.Read</code>. - If not, MSAL requests a new access token with the required scope. - Access token is not altered by claim transformations.</p>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#sequence-diagram-authentication-and-api-call-flow","title":"Sequence Diagram: Authentication and API Call Flow","text":"<pre><code>sequenceDiagram\n    participant UI as Blazor UI\n    participant Blazor as Blazor Server\n    participant Entra as Entra ID (Azure AD)\n    participant Gateway as WSO2 API Manager\n    participant API as Downstream API / Graph\n    UI -&gt;&gt; Blazor: User signs in\n    Blazor -&gt;&gt; Entra: Auth request with scopes (User.Read, Mail.Read)\n    Entra --&gt;&gt; Blazor: ID Token + Access Token\n    Blazor -&gt;&gt; Blazor: Build ClaimsPrincipal (apply IClaimsTransformation)\n    UI -&gt;&gt; Blazor: Request data via GraphComponent\n    Blazor -&gt;&gt; Entra: Acquire Access Token for Graph (User.Read)\n    Entra --&gt;&gt; Blazor: Access Token with scp=User.Read\n    Blazor -&gt;&gt; Gateway: Call API with Bearer token\n    Gateway -&gt;&gt; Entra: Validate token (signature, issuer, scopes)\n    Gateway -&gt;&gt; API: Forward request if valid\n    API --&gt;&gt; Gateway: Response\n    Gateway --&gt;&gt; Blazor: Response\n    Blazor --&gt;&gt; UI: Render data (e.g., displayName)</code></pre>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#additional-considerations-with-wso2-api-manager","title":"Additional Considerations with WSO2 API Manager","text":"<p>When using WSO2 API Manager (APIM) as the gateway:</p>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#token-validation-in-wso2","title":"Token Validation in WSO2","text":"<ul> <li>WSO2 expects self-contained JWT access tokens.</li> <li>Validates:</li> <li>Signature against trusted public keys.</li> <li>Standard claims (<code>iss</code>, <code>sub</code>, <code>exp</code>, <code>iat</code>).</li> <li>Scopes (<code>scp</code> or <code>scope</code>) for API access.</li> </ul>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#claim-mapping-and-scope-translation","title":"Claim Mapping and Scope Translation","text":"<ul> <li>Entra tokens use <code>scp</code> for scopes; WSO2 may expect <code>scope</code>.</li> <li>Use claim mapping in WSO2 to align claims.</li> <li>Scope enforcement is configurable per API definition.</li> </ul>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#azure-ad-integration","title":"Azure AD Integration","text":"<ul> <li>Azure AD is not natively supported as a WSO2 Key Manager.</li> <li>Options:</li> <li>Implement a custom key manager in WSO2 to validate Entra tokens directly.</li> <li>Or use a token exchange flow (exchange Entra-issued token for a WSO2-recognized token).</li> </ul>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#claim-based-access-policies","title":"Claim-Based Access Policies","text":"<ul> <li>WSO2 supports JWT claim-based access validators.</li> <li>You can enforce policies like:</li> <li>Restrict access to users from specific tenant IDs.</li> <li>Require a role claim (e.g., <code>roles: Admin</code>).</li> </ul>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#backend-authentication","title":"Backend Authentication","text":"<ul> <li>WSO2 can manage its own OAuth2 credentials when authenticating to backends.</li> <li>Use OAuth2 endpoint security with grant types such as client credentials.</li> <li>Supports Redis caching for OAuth state across distributed gateways.</li> </ul>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#scope-allowlisting","title":"Scope Allowlisting","text":"<ul> <li>Certain scopes can be allowlisted to bypass role checks.</li> <li>Configure allowlisting in <code>deployment.toml</code> if some scopes should always pass validation.</li> </ul>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Request least-privilege scopes \u2013 never ask for more than you need.</li> <li>Enforce scope checks at WSO2 gateway/API using the <code>scp</code> or <code>scope</code> claim.</li> <li>Use claims transformations only locally \u2013 do not rely on them for API security.</li> <li>Validate issuer and audience in every API/gateway.</li> <li>Rely on MSAL for token refresh and caching \u2013 do not store tokens manually.</li> <li>Use <code>[AuthorizeForScopes]</code> to ensure tokens have necessary permissions before API calls.</li> <li>Configure WSO2 to:</li> <li>Trust Entra public signing keys.</li> <li>Map claims as needed (e.g., <code>scp</code> \u2192 <code>scope</code>).</li> <li>Enforce claim-based access control for sensitive APIs.</li> </ol>","tags":["design"]},{"location":"blog/2025/08/29/scopes-claims-and-token-behavior-in-blazor-server-with-entra-id-and-microsoft-graph/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Scopes = Permissions (what you can do, validated by APIs/gateway).</li> <li>Claims = Identity attributes (who you are, used by apps for authorization/UI).</li> <li>ID Token = Establishes user identity.</li> <li>Access Token = Grants permissions to call APIs.</li> <li>Refresh Token = Silently gets new access tokens.</li> <li>Claims transformations only affect local app logic, not tokens sent downstream.</li> <li>Authority/Issuer ensure tokens are trusted and from the correct tenant.</li> <li>WSO2 APIM must be configured to:</li> <li>Accept and validate Entra-issued JWTs.</li> <li>Enforce scopes and claims consistently.</li> <li>Optionally use token exchange or custom key manager if Entra tokens are not directly consumable.</li> </ul> <p>\ud83d\udccc By clearly separating scope enforcement (at WSO2 gateway/API) from claims usage (inside Blazor) and understanding token lifecycles, you can build secure Blazor Server applications aligned with enterprise security policies.</p>","tags":["design"]},{"location":"blog/2023/11/03/avoiding-flaky-tests-with-timeprovider-and-itimer/","title":"Avoiding flaky tests with TimeProvider and ITimer","text":"<p>This is an extract from Andrew Lock's blog.  Jump over and check out his work, he is a very talented individual and has some awesome posts.</p>","tags":["testing","TimeProvider"]},{"location":"blog/2023/11/03/avoiding-flaky-tests-with-timeprovider-and-itimer/#whats-the-problem-with-the-existing-apis","title":"What's the problem with the existing APIs?","text":"<p>Even if you're new to C#, you have likely needed to get \"the current time\" at some point. The default way to do that is to use the <code>DateTime.UtcNow</code> or <code>DateTimeOffset.UtcNow</code> static properties. You can call these properties from anywhere in your code so they're an easy thing to reach for wherever you are.</p> Info <p>In this post I'm not going to get into the differences between <code>DateTime</code>/<code>DateTimeOffset</code>, <code>Now</code>/<code>UtcNow</code>, or whether you should actually be using something like NodaTime. If you aren't aware of that project you should definitely look at it!</p> <p>Grabbing the current time this way can be fine, right up until you need to test something. Lets take a simple concrete example. Imagine you're modelling a \"support ticket\" system, which has the following rules to apply when a new ticket is created.</p> <ul> <li>Record the UTC time the ticket was opened</li> <li>If opened by a standard level customer, automatically close the ticket after 7 days.</li> <li>It opened by a gold level customer, automatically close the ticket after 14 days.</li> </ul> <pre><code>public class Ticket\n{\n    private readonly DateTime _createdDate;\n    private readonly DateTime _expiresDate;\n    public Ticket(Level level)\n    {\n        _createdDate = DateTime.UtcNow;\n        var expiresAfter = level == Level.Gold ? 14 : 7;\n        _expiresDate = _createdDate.AddDays(expiresAfter);\n    }\n\n    public bool IsExpired() =&gt; _expiresDate &gt; DateTime.UtcNow;\n}\n\npublic enum Level\n{\n    Standard,\n    Gold,\n}\n</code></pre> <p>This is a naively written type, but lets say you wanted to test it. Currently the only way to test this code is to wait 14 days to make sure the Ticket has expired! There's a very simple solution to this: pass the current date in as a parameter of the methods instead:</p> <pre><code>public class Ticket\n{\n    private readonly DateTime _createdDate;\n    private readonly DateTime _expiresDate;\n    public Ticket(Level level, DateTime createdDate)\n    {\n        _createdDate = createdDate;\n        var expiresAfter = level == Level.Gold ? 14 : 7;\n        _expiresDate = _createdDate.AddDays(expiresAfter);\n    }\n\n    public bool IsExpired(DateTime now) =&gt; _expiresDate &gt; now;\n}\n</code></pre> <p>This code is now easily testable; you can make sure each Level of customer has the expected expiry date by passing in different values for <code>now</code> into <code>IsExpired()</code>. But what if this test is part of an integration test, so you can't directly manipulate those method arguments?</p> <p>Another distinct use case is when you want to test something that runs in the background on some sort of timer. For example, consider the following toy example that increments an integer every second:</p> <pre><code>public class MyBackgroundWorker : IAsyncDisposable\n{\n    private readonly TaskCompletionSource&lt;bool&gt; _processExit = new();\n    private readonly Task _loopTask;\n\n    public int Value { get; set; }\n    public DateTimeOffset LastUpdate {get;set;}\n\n    public MyBackgroundWorker()\n    {\n        _loopTask = Task.Run(RunLoopAsync);\n    }\n\n    public async ValueTask DisposeAsync()\n    {\n        _processExit.TrySetResult(true);\n        await _loopTask;\n    }\n\n    async Task RunLoopAsync()\n    {\n        while (!_processExit.Task.IsCompleted)\n        {\n            // Wait for the delay to expire or disposal\n            var delay = Task.Delay(TimeSpan.FromSeconds(1));\n            await Task.WhenAny(delay, _processExit.Task);\n\n            Value++;   \n            LastUpdate = DateTimeOffset.UtcNow;\n        }\n    }\n}\n</code></pre> <p>The details of this aren't very important; the key is that we're using a <code>Task.Delay</code> to increment the integer periodically. We could also have used a <code>Timer</code>, but the overall design is much the same.</p> <p>Lets say we want to test this. We have a couple of problems:</p> <ul> <li>The timespan is fixed. That will make for slow tests. We could control this by injecting in the delay to use in the constructor, but maybe we don't really want it to be configurable, and it's important that it's always 1 second</li> <li>Using timers in tests often causes flaky tests. Even if we allow injecting the delay in the constructor, the runtime doesn't guarantee that exactly the requested time will pass for a <code>Task.Delay()</code> call, just that at least that time will pass. This is often noticeable in CI where machines are working at maximum capacity; timers don't fire as regularly as you expect, and so tests fail and flake. You can work around this by, for example, adding callbacks inside the loop, but now you're making a lot of changes and adding a lot of API surface you don't necessarily want to expose.</li> </ul> <p>For example, to check that after 5 iterations we have the correct value, our current best effort might look something like this:</p> <pre><code>public class WorkerTests\n{\n    [Fact]\n    public async Task BackgroundWorkerRunsPeriodically()\n    {\n        var delay = TimeSpan.FromSeconds(5);\n        var expectedLastUpdate = DateTimeOffset.UtcNow.Add(delay);\n\n        var worker = new MyBackgroundWorker();\n\n        // Nominally wait 5s (i.e. 5x1s loop)\n        await Task.Delay(delay); \n        await worker.DisposeAsync();\n\n        // Realistically, this might be 4, 5, 6, or \ud83d\ude05\n        worker.Value.Should().Be(5);\n        // Who knows, this will probably be very flaky \n        worker.LastUpdate.Should().BeCloseTo(expectedLastUpdate, TimeSpan.FromMilliseconds(100));\n    }\n}\n</code></pre> <p>Tests like this will always be slow (it takes at least 5s to run) and horribly flaky.</p> <p>Neither of the design problems I described are insurmountable. We can remove the flake by taking the concerns into account when we design our classes, but they require us to change our implementations, potentially significantly. The new <code>TimeProvider</code> abstraction added to .NET 8 provides an alternative solution.</p>","tags":["testing","TimeProvider"]},{"location":"blog/2023/11/03/avoiding-flaky-tests-with-timeprovider-and-itimer/#the-timeprovider-abstraction-in-net-8","title":"The TimeProvider abstraction in .NET 8","text":"<p>Adding a \"time\" abstraction that wraps calls to <code>DateTime.UtcNow</code> and other similar APIs has been a common approach to testing for a long time. I've seen such abstractions (often called <code>IClock</code>, <code>ISystemClock</code>, or <code>IDateTimeProvider</code> etc) that look something like this:</p> <pre><code>public interface IClock\n{\n    DateTime UtcNow { get; }\n}\n</code></pre> <p>These abstractions can have simple \"default\" implementations in your app code:</p> <pre><code>public class SystemClock : IClock\n{\n    public DateTime UtcNow =&gt; DateTime.UtcNow;\n}\n</code></pre> <p>but in your tests, you can create fakes that carefully control the values returned as required by specific tests.</p> <p>.NET 8 introduced a similar abstraction, an abstract base class, called <code>TimeProvider</code>:</p> <pre><code>public abstract class TimeProvider\n{\n    public static TimeProvider System { get; }\n\n    protected TimeProvider();\n\n    public virtual TimeZoneInfo LocalTimeZone { get; }\n    public virtual long TimestampFrequency { get; }\n\n    public DateTimeOffset GetLocalNow();\n    public virtual DateTimeOffset GetUtcNow();\n    public virtual long GetTimestamp();\n    public TimeSpan GetElapsedTime(long startingTimestamp);\n    public TimeSpan GetElapsedTime(long startingTimestamp, long endingTimestamp);\n\n    public virtual ITimer CreateTimer(TimerCallback callback, object? state,TimeSpan dueTime, TimeSpan period);\n}\n</code></pre> <p>This abstraction is similar to the basic <code>IClock</code> I showed earlier in that it has the <code>GetUtcNow()</code> method, and the <code>static System</code> property exposes the default <code>SystemTimeProvider</code> implementation that you'll inject into your app in normal operation.</p> <p>But this abstraction is much bigger than <code>IClock</code>, or any other time abstraction I have used before! <code>TimeProvider</code> seems to include everything and the kitchen sink\u2014it not only has methods for getting the local or UTC time now, it has methods for comparing timestamps, and even for creating timers! On first thought, that seems like overkill, like a mixing of concerns, but (unsurprisingly) there's a very good reason for it.</p> <p>The simple <code>IClock</code> abstraction allows you to remove the implicit dependency on the system time (i.e. <code>DateTime.UtcNow</code> calls) from your own code, but it doesn't do anything about the implicit dependency on the passage of time in the .NET base class library calls. Things like <code>Task.Delay()</code> have an implicit dependency on the system time, because they essentially track the difference between two times.</p>","tags":["testing","TimeProvider"]},{"location":"blog/2023/11/03/avoiding-flaky-tests-with-timeprovider-and-itimer/#the-itimer-abstraction-in-net-8","title":"The ITimer abstraction in .NET 8","text":"<p>One of the important methods on the <code>TimeProvider</code> abstraction in .NET 8 is <code>CreateTimer()</code>. This method means you can now fully test your <code>Timer</code> or <code>Task.Delay</code>-based code by refactoring it to use <code>TimeProvider</code> instead. The <code>ITimer</code> created by <code>CreateTimer()</code> is tied to the <code>TimeProvider</code> so that the timers trigger as time advances.</p> <p>What's more, a bunch of overloads have been added to types like Task to allow passing in a <code>TimeProvider</code> instance, for example:</p> <pre><code>public partial class Task : IAsyncResult, IDisposable\n{\n    public static Task Delay(TimeSpan delay, TimeProvider timeProvider)\n}\n\npublic partial class Task&lt;TResult&gt; : Task\n{\n    public new Task&lt;TResult&gt; WaitAsync(TimeSpan timeout, TimeProvider timeProvider)\n}\n</code></pre> <p>What's more, a new NuGet package, Microsoft.Bcl.TimeProvider backports these APIs all the way back to <code>netstandard2.0</code> (and .NET Framework) by adding extension method equivalents (as well as the <code>TimeProvider</code> and <code>ITimer</code> implementations themselves):</p> <pre><code>public static class TimeProviderTaskExtensions\n{\n    public static Task Delay(this TimeProvider timeProvider, TimeSpan delay) \n    public static Task&lt;TResult&gt; WaitAsync&lt;TResult&gt;(this Task&lt;TResult&gt; task, TimeSpan timeout, TimeProvider timeProvider)\n}\n</code></pre> <p>So with all that in mind, lets re-write the <code>MyBackgroundWorker</code> to use the <code>TimeProvider</code> abstraction and the timer-related overloads.</p> <pre><code>public class MyBackgroundWorker : IAsyncDisposable\n{\n    // Take a dependency on TimeProvider \ud83d\udc47\n    private readonly TimeProvider _timeProvider;\n    private readonly TaskCompletionSource&lt;bool&gt; _processExit = new();\n    private readonly Task _loopTask;\n\n    public int Value { get; set; }\n    public DateTimeOffset LastUpdate {get;set;}\n\n    // Inject the TimeProvider \ud83d\udc47\n    public MyBackgroundWorker(TimeProvider timeProvider)\n    {\n        _timeProvider = timeProvider;\n        _loopTask = Task.Run(RunLoopAsync);\n    }\n\n    public async ValueTask DisposeAsync()\n    {\n        _processExit.TrySetResult(true);\n        await _loopTask;\n    }\n\n    async Task RunLoopAsync()\n    {\n        while (!_processExit.Task.IsCompleted)\n        {\n                            // Pass TimeProvider into Task.Delay \ud83d\udc47\n            var delay = Task.Delay(TimeSpan.FromSeconds(1), _timeProvider);\n            await Task.WhenAny(delay, _processExit.Task);\n\n            Value++;   \n            // \ud83d\udc47 Use TimeProvider to get the current time \n            LastUpdate = _timeProvider.GetUtcNow();\n        }\n    }\n}\n</code></pre> <p>If we pass the system TimeProvider into our class, then it behaves exactly as it did before. But now, crucially, we can easily (quickly and reliably) test our class, without having to change its public API.</p>","tags":["testing","TimeProvider"]},{"location":"blog/2023/11/03/avoiding-flaky-tests-with-timeprovider-and-itimer/#testing-with-the-microsoftextensionstimeprovidertesting-library","title":"Testing with the Microsoft.Extensions.TimeProvider.Testing library","text":"<p>As well as the Microsoft.Bcl.TimeProvider NuGet package, Microsoft have also created the Microsoft.Extensions.TimeProvider.Testing package for testing. This includes a <code>TimeProvider</code> implementation called <code>FakeTimeProvider</code>.</p> <p><code>FakeTimeProvider</code> is a time provider in which you control the flow of time. You can specify a specific start time for the <code>FakeTimeProvider</code> and it will act as if that's the current time. You can then call <code>Advance(TimeSpan)</code> to advance time forward. After advancing, any <code>ITimer</code>s created by the provider will trigger, and the new time will be returned.</p> <p>All this means that it's relatively easy to create tests thatW test types that rely on timers and accessing the system time, as long as these types use the <code>TimeProvider</code> abstraction. For example, we could re-write our background worker test to the following:</p> <pre><code>public class WorkerTests\n{\n    [Fact]\n    public async Task BackgroundWorkerRunsPeriodically()\n    {\n        // We don't care about the start time in this case, but\n        // you can set it if you need to\n        var fakeTime = new FakeTimeProvider(startDateTime: DateTimeOffset.UtcNow);\n\n        // Pass in the FakeTimeProvider as a TimeProvider\n        var worker = new MyBackgroundWorker(fakeTime);\n\n        // Advance 0.5s, the timer should not have fired yet (fires every 1s)\n        fakeTime.Advance(TimeSpan.FromMilliseconds(500));\n        worker.Value.Should().Be(0);\n\n        // Forward 1.5s, this will fire the timer twice, but our loop code will only execute once\n        fakeTime.Advance(TimeSpan.FromMilliseconds(1_500));\n        worker.Value.Should().Be(1); // looped once\n        worker.LastUpdate.Should().Be(fakeTime.Start.AddSeconds(2)); // total expired time\n\n        // Forward 1s, we're at 3s total, so the timer should have have fired again\n        fakeTime.Advance(TimeSpan.FromSeconds(1));\n        worker.Value.Should().Be(2); // looped again\n        worker.LastUpdate.Should().Be(fakeTime.Start.AddSeconds(3)); // fired at 3s\n\n        await worker.DisposeAsync();\n\n        // Should have looped one more time on exit\n        worker.Value.Should().Be(3);\n    }\n}\n</code></pre> <p>This test code shows a couple of features:</p> <ul> <li>We can advance the clock manually by calling <code>Advance()</code>. This controls the value of <code>GetUtcNow()</code> and other related methods.</li> <li>Advancing the clock triggered the <code>ITimer</code>s to fire if they have expired. Note that for this example the <code>FakeTimeProvider</code> does not \"interrupt\" the <code>Advance</code> to fire the timer used by <code>Task.Delay</code> after 1s; it fires the timer twice at the end of the <code>Advance()</code> call. However, the <code>Task.Delay()</code> call only \"cares\" about the first firing, so the second one is \"ignored\". So we only loop once, and only increment <code>Value</code> once, even though we theoretically would expect to fire twice for the time we advanced.</li> </ul> <p>Note</p> <p>Another cool feature of the <code>FakeTimeProvider</code> is <code>AutoAdvance</code>. This lets you automatically advance the current time every time someone calls <code>GetUtcNow()</code>.</p> <p>Even with the control <code>FakeTimeProvider</code> gives, it's still easy to make mistakes with timers.Even ignoring the oddity around large time advancements, there's actually still a race condition in our test above: our test code might call the first <code>Advance()</code> in the test code before the <code>Task.Delay()</code> call in the worker, in which case the test above will fail. One possible solution is to simplify the worker implementation to make direct use of the <code>ITimer</code> abstraction:</p> <pre><code>public class MyBackgroundWorker : IAsyncDisposable\n{\n    private readonly TimeProvider _timeProvider;\n    private readonly ITimer _timer;\n\n    public int Value { get; set; }\n    public DateTimeOffset LastUpdate {get;set;}\n\n    public MyBackgroundWorker(TimeProvider timeProvider)\n    {\n        _timeProvider = timeProvider;\n\n        // Create a timer directly from the provider\n        _timer = timeProvider.CreateTimer(\n            callback: UpdateTotals,\n            state: this, \n            dueTime: TimeSpan.FromSeconds(1), \n            period: TimeSpan.FromSeconds(1));\n    }\n\n    private static void UpdateTotals(object? state)\n    {\n        // Update the values\n        var worker = (MyBackgroundWorker) state!;\n        worker.Value++;\n        worker.LastUpdate = worker._timeProvider.GetUtcNow();\n    }\n\n    public ValueTask DisposeAsync() =&gt; _timer.DisposeAsync();\n}\n</code></pre> <p>This implementation is objectively simpler, and as it uses the <code>CreateTimer()</code> method directly, it doesn't suffer from the same race condition as the previous implementation. What's more, as we're providing a callback to the <code>ITimer</code> directly (instead of indirectly using <code>Task.Delay()</code>), our test now does behave correctly when we advance multiple intervals. So if we advance by 3s, our callback will fire 3 times (instead of just once, as in the previous implementation). Lets update the test to account for that:</p> <pre><code>public class WorkerTests\n{\n    [Fact]\n    public async Task BackgroundWorkerRunsPeriodically()\n    {\n        var fakeTime = new FakeTimeProvider(startDateTime: DateTimeOffset.UtcNow);\n        var worker = new MyBackgroundWorker(fakeTime);\n        await Task.Yield();\n\n        // Advance 0.5s, the timer should not have fired yet\n        fakeTime.Advance(TimeSpan.FromMilliseconds(500));\n        worker.Value.Should().Be(0);\n\n        // Advance another 0.5s and the timer fires!\n        fakeTime.Advance(TimeSpan.FromMilliseconds(500));\n        worker.Value.Should().Be(1);\n        worker.LastUpdate.Should().Be(fakeTime.Start.AddSeconds(1)); // total expired time\n\n        // Forward 2s, this fires twice, because we used the ITimer directly\n        fakeTime.Advance(TimeSpan.FromSeconds(2));\n        worker.Value.Should().Be(3); // fired twice\n        worker.LastUpdate.Should().Be(fakeTime.Start.AddSeconds(3)); // total expired time\n\n        // Forward 1s, we're at 4s total, so the timer should have have fired again\n        fakeTime.Advance(TimeSpan.FromSeconds(1));\n        worker.Value.Should().Be(4); // fired again\n        worker.LastUpdate.Should().Be(fakeTime.Start.AddSeconds(4)); // final time\n\n        await worker.DisposeAsync();\n    }\n}\n</code></pre> <p>And there we have it! A deterministic test for a class that uses timers. We didn't need to change the public API to test it (outside of using TimeProvider), and yet we can run detailed time-based tests, controlling the flow of time. I feel like this is a game changer for testability, TimeProvider is my new best friend when writing this sort of code!</p>","tags":["testing","TimeProvider"]},{"location":"blog/2023/11/03/avoiding-flaky-tests-with-timeprovider-and-itimer/#summary","title":"Summary","text":"<p>In this post I described some of the reasons a time abstraction can be both useful and/or necessary if you want to test your applications. .NET 8 adds a new abstraction (and implementation) called <code>TimeProvider</code> that serves this purpose. As well as providing an abstraction equivalent of <code>DateTimeOffset.UtcNow</code>, it also allows creating <code>ITimer</code> timer implementations. Moreover, many base library types like <code>Task</code> have been updated to work with <code>TimeProvider</code> and <code>ITimer</code>. These abstractions have even been backported to <code>netstandard2.0</code> and .NET Framework.</p> <p>In addition to the abstraction, a testing package, Microsoft.Extensions.TimeProvider.Testing, has been created. This provides a <code>FakeTimeProvider</code> which can be used to control the flow of time. This makes it easy to test all your types that depend on the system time or timers internally!</p>","tags":["testing","TimeProvider"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/dotnet/","title":"dotnet","text":""},{"location":"blog/category/minimal-api/","title":"minimal api","text":""},{"location":"blog/category/docker/","title":"docker","text":""},{"location":"blog/category/security/","title":"security","text":""},{"location":"blog/category/programming/","title":"programming","text":""},{"location":"blog/category/ai/","title":"ai","text":""},{"location":"blog/category/git/","title":"git","text":""},{"location":"blog/category/mediator/","title":"mediator","text":""},{"location":"blog/category/modeller/","title":"modeller","text":""},{"location":"blog/category/codegen/","title":"codegen","text":""},{"location":"blog/category/net-8/","title":".net 8","text":""},{"location":"blog/category/masstransit/","title":"masstransit","text":""},{"location":"blog/category/rabbitmq/","title":"rabbitmq","text":""},{"location":"blog/page/2/","title":"Blog","text":""}]}